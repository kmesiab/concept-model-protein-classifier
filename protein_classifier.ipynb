{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b534985e-b516-41af-b343-14da11c248bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Extracted 15000 chains → 'pdb_chains.fasta'\n"
     ]
    }
   ],
   "source": [
    "# 1.) Download the PDB chain sequences (FASTA format from RCSB) via the HTTPS mirror,\n",
    "#      then keep only the first 15 000 entries.\n",
    "\n",
    "import requests\n",
    "\n",
    "# Use the “files.wwpdb.org” HTTPS mirror instead of FTP\n",
    "pdb_url = \"https://files.wwpdb.org/pub/pdb/derived_data/pdb_seqres.txt\"\n",
    "try:\n",
    "    resp = requests.get(pdb_url, timeout=60)\n",
    "    resp.raise_for_status()\n",
    "    text = resp.text.strip()\n",
    "    if not text.startswith(\">\"):\n",
    "        raise RuntimeError(\"Downloaded content does not look like FASTA.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to download PDB chain sequences: {e}\")\n",
    "\n",
    "# Write the complete dump to a temporary file\n",
    "with open(\"pdb_chains.fasta\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text + \"\\n\")\n",
    "\n",
    "# ─── Split the full FASTA into individual (header, sequence) tuples ─────────────\n",
    "def split_fasta(filepath):\n",
    "    sequences = []\n",
    "    with open(filepath, \"r\") as f:\n",
    "        header = None\n",
    "        seq_lines = []\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if line.startswith(\">\"):\n",
    "                if header is not None:\n",
    "                    sequences.append((header, \"\".join(seq_lines)))\n",
    "                header = line\n",
    "                seq_lines = []\n",
    "            else:\n",
    "                seq_lines.append(line)\n",
    "        # Add the final sequence\n",
    "        if header is not None:\n",
    "            sequences.append((header, \"\".join(seq_lines)))\n",
    "    return sequences\n",
    "\n",
    "all_chains = split_fasta(\"pdb_chains.fasta\")\n",
    "\n",
    "# ─── Keep exactly the first 15 000 chains ─────────────────────────────────────────\n",
    "subset = all_chains[:15000]\n",
    "\n",
    "# ─── Write those 15 000 chains back to “pdb_chains.fasta” ───────────────────────\n",
    "with open(\"pdb_chains.fasta\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for header, seq in subset:\n",
    "        f.write(f\"{header}\\n\")\n",
    "        f.write(f\"{seq}\\n\")\n",
    "\n",
    "print(f\"✔ Extracted {len(subset)} chains → 'pdb_chains.fasta'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e30de29-e8f5-4fe7-abfd-58594dc600e6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Successfully fetched 100 DisProt sequences in FASTA format → 'disprot_1000.fasta'\n"
     ]
    }
   ],
   "source": [
    "# 2.) Use DisProt’s search endpoint with format=fasta\n",
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"https://disprot.org/api/search?format=fasta&limit=10000\"\n",
    "try:\n",
    "    resp = requests.get(url, timeout=15)\n",
    "    resp.raise_for_status()\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to GET DisProt FASTA via API: {e}\")\n",
    "\n",
    "text = resp.text.strip()\n",
    "\n",
    "# 2.2) Quick sanity check: FASTA must start with '>', not '<'\n",
    "if not text.startswith(\">\"):\n",
    "    raise RuntimeError(\n",
    "        \"Downloaded content does not look like FASTA. \"\n",
    "        \"If it begins with '<', you're still hitting an HTML page instead of raw FASTA.\"\n",
    "    )\n",
    "\n",
    "# 2.3) Write the 100 DisProt entries to a file\n",
    "with open(\"disprot_13000.fasta\", \"w\") as f:\n",
    "    f.write(text + \"\\n\")\n",
    "\n",
    "print(\"✔ Successfully fetched 100 DisProt sequences in FASTA format → 'disprot_1000.fasta'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b9b660b-dd1e-48ac-a9d2-8dc830cae3d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Fetched 25000 DisProt sequences → 'disprot_13000.fasta'\n"
     ]
    }
   ],
   "source": [
    "# 2.1) Collect more data\n",
    "\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# ─── PARAMETERS ─────────────────────────────────────────────────────────────\n",
    "TOTAL_DESIRED = 25_000   # how many DisProt sequences we want total\n",
    "PER_PAGE      = 100      # DisProt’s hard cap per request\n",
    "OUTPUT_FILE   = \"disprot_13000.fasta\"\n",
    "\n",
    "accum_seqs = []\n",
    "offset     = 0\n",
    "\n",
    "while len(accum_seqs) < TOTAL_DESIRED:\n",
    "    url = f\"https://disprot.org/api/search?format=fasta&limit={PER_PAGE}&offset={offset}\"\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to GET DisProt FASTA (offset={offset}): {e}\")\n",
    "\n",
    "    block = resp.text.strip()\n",
    "    if not block.startswith(\">\"):\n",
    "        raise RuntimeError(\n",
    "            \"Downloaded content does not look like FASTA. \"\n",
    "            \"If it begins with '<', you're still hitting an HTML page.\"\n",
    "        )\n",
    "\n",
    "    # Parse out this page’s FASTA sequences (collecting only the raw sequences, not full headers):\n",
    "    raw_lines = block.splitlines()\n",
    "    header = None\n",
    "    seq_buf = \"\"\n",
    "    this_page_seqs = []\n",
    "    for line in raw_lines:\n",
    "        if line.startswith(\">\"):\n",
    "            if header is not None and seq_buf:\n",
    "                this_page_seqs.append(seq_buf)\n",
    "            header = line\n",
    "            seq_buf = \"\"\n",
    "        else:\n",
    "            seq_buf += line.strip()\n",
    "    if header is not None and seq_buf:\n",
    "        this_page_seqs.append(seq_buf)\n",
    "\n",
    "    if not this_page_seqs:\n",
    "        # No more sequences returned → break out early\n",
    "        break\n",
    "\n",
    "    accum_seqs.extend(this_page_seqs)\n",
    "    offset += PER_PAGE\n",
    "\n",
    "    # Sleep briefly (so we don’t hammer the server)\n",
    "    time.sleep(0.4)\n",
    "\n",
    "# Trim in case we overshot\n",
    "accum_seqs = accum_seqs[:TOTAL_DESIRED]\n",
    "\n",
    "# Write out ~25k sequences in FASTA format (with minimal headers)\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    for i, seq in enumerate(accum_seqs):\n",
    "        f.write(f\">disprot_sequence_{i+1}\\n\")\n",
    "        f.write(seq + \"\\n\")\n",
    "\n",
    "print(f\"✔ Fetched {len(accum_seqs)} DisProt sequences → '{OUTPUT_FILE}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4177fdf8-80da-43a8-8fa1-2633860ed0cb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">disprot_sequence_1\n",
      "EHVIEMDVTSENGQRALKEQSSKAKIVKNRWGRNVVQISNT\n",
      ">disprot_sequence_2\n",
      "VYRNSRAQGGG\n",
      ">disprot_sequence_3\n"
     ]
    }
   ],
   "source": [
    "# 2.2) Verify Downloaded Sequences\n",
    "with open(\"disprot_13000.fasta\") as f:\n",
    "    for _ in range(5):\n",
    "        print(f.readline().rstrip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adffd98f-0799-4b9e-9883-c790a96efe60",
   "metadata": {},
   "source": [
    "# Sliding Window vs Global Learned Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a75ce8de-d866-4859-861f-81bc69fef95e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15000 PDB sequences.\n",
      "Loaded 25000 DisProt sequences.\n",
      "\n",
      "Computing NEW GLOBAL features (WINDOW_SIZE_GLOBAL_FEATURES = None)...\n",
      "NEW GLOBAL feature computation complete.\n",
      "\n",
      "Global Features Training set size: 32000\n",
      "Global Features Testing set size: 8000\n",
      "\n",
      "Global Feature Means (DisProt vs. PDB) from NEW GLOBAL FEATURES TRAINING DATA:\n",
      "\n",
      "         hydro_norm_avg  flex_norm_avg  h_bond_potential_avg  \\\n",
      "label                                                          \n",
      "DisProt        0.401099       0.837249              1.256680   \n",
      "PDB            0.475068       0.806553              1.073287   \n",
      "\n",
      "         abs_net_charge_prop  shannon_entropy  freq_proline  \\\n",
      "label                                                         \n",
      "DisProt             0.115245         3.359850      0.069069   \n",
      "PDB                 0.036236         3.700737      0.043093   \n",
      "\n",
      "         freq_bulky_hydrophobics  \n",
      "label                             \n",
      "DisProt                 0.210505  \n",
      "PDB                     0.312975   \n",
      "\n",
      "Chosen Midpoint Thresholds (from NEW GLOBAL FEATURES TRAINING DATA):\n",
      "\n",
      "  hydro_norm_avg     = 0.438\n",
      "  flex_norm_avg      = 0.822\n",
      "  h_bond_potential_avg = 1.165\n",
      "  abs_net_charge_prop = 0.076\n",
      "  shannon_entropy    = 3.530\n",
      "  freq_proline       = 0.056\n",
      "  freq_bulky_hydrophobics = 0.262\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluating New Global Features Threshold-Based Classifier ---\n",
      "Distribution of ‘conditions_met’ (NEW GLOBAL features) by Label (ON TEST SET):\n",
      "\n",
      "conditions_met    0     1     2     3    4    5     6    7\n",
      "label                                                     \n",
      "DisProt         299  1014  1205  1034  716  469   209   54\n",
      "PDB               6    28    49    90  333  563  1074  857 \n",
      "\n",
      "Performance of New Global Features Classifier on TEST SET (varying k):\n",
      "\n",
      " k   TP   FN   TN   FP Accuracy Precision (PDB) Recall (PDB) F1-score (PDB)\n",
      " 1 2994    6  299 4701   41.16%          38.91%       99.80%         55.99%\n",
      " 2 2966   34 1313 3687   53.49%          44.58%       98.87%         61.45%\n",
      " 3 2917   83 2518 2482   67.94%          54.03%       97.23%         69.46%\n",
      " 4 2827  173 3552 1448   79.74%          66.13%       94.23%         77.72%\n",
      " 5 2494  506 4268  732   84.52%          77.31%       83.13%         80.12%\n",
      " 6 1931 1069 4737  263   83.35%          88.01%       64.37%         74.36%\n",
      " 7  857 2143 4946   54   72.54%          94.07%       28.57%         43.83%\n",
      "\n",
      "--- Detailed New Global Features Classification Report for best k = 5 (based on F1 PDB) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " DisProt (0)       0.89      0.85      0.87      5000\n",
      "     PDB (1)       0.77      0.83      0.80      3000\n",
      "\n",
      "    accuracy                           0.85      8000\n",
      "   macro avg       0.83      0.84      0.84      8000\n",
      "weighted avg       0.85      0.85      0.85      8000\n",
      "\n",
      "Confusion Matrix for best k (New Global Features):\n",
      "                 Pred DisProt  Pred PDB\n",
      "Actual DisProt          4268       732\n",
      "Actual PDB               506      2494\n",
      "\n",
      "\n",
      "--- Testing Sliding Window (Larger) Classifier with Failure Cancellation ---\n",
      "\n",
      "Applying Sliding Window (Larger) Classifier (NEW features) with Failure Cancellation: window_size=9, slide_step=9, window_k_pass_thresh=4, max_total_unforgiven_failures=3...\n",
      "\n",
      "Sliding Window (Larger) with Failure Cancellation classification complete.\n",
      "\n",
      "Performance of Sliding Window (Larger) Classifier with Failure Cancellation (ON TEST SET):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " DisProt (0)       0.86      0.18      0.30      5000\n",
      "     PDB (1)       0.41      0.95      0.57      3000\n",
      "\n",
      "    accuracy                           0.47      8000\n",
      "   macro avg       0.64      0.57      0.44      8000\n",
      "weighted avg       0.69      0.47      0.40      8000\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Pred DisProt  Pred PDB\n",
      "Actual DisProt           910      4090\n",
      "Actual PDB               149      2851\n",
      "Accuracy: 47.01%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "\n",
    "# --- Parameters for Classifiers ---\n",
    "# For Global Feature Classifier (WINDOW_SIZE_GLOBAL_FEATURES = None means direct global calculation)\n",
    "WINDOW_SIZE_GLOBAL_FEATURES = None \n",
    "\n",
    "# For Sliding Window Classifier\n",
    "SLIDING_WINDOW_SIZE = 9  # Size of the sliding window\n",
    "SLIDING_WINDOW_SLIDE_STEP = 9 # Step for sliding (equal to WINDOW_SIZE for non-overlapping)\n",
    "SLIDING_WINDOW_PASS_K = 4     # Min conditions a window must meet to \"pass\"\n",
    "MAX_UNFORGIVEN_FAILED_WINDOWS_SLIDING = 3 # Max uncancelled failed windows for protein to pass\n",
    "\n",
    "# ─── (A) Build aa_properties (underlying single AA properties) ────────────────\n",
    "kd_hydro = {\n",
    "    'A':  1.8, 'R': -4.5, 'N': -3.5, 'D': -3.5, 'C':  2.5,\n",
    "    'Q': -3.5, 'E': -3.5, 'G': -0.4, 'H': -3.2, 'I':  4.5,\n",
    "    'L':  3.8, 'K': -3.9, 'M':  1.9, 'F':  2.8, 'P': -1.6,\n",
    "    'S': -0.8, 'T': -0.7, 'W': -0.9, 'Y': -1.3, 'V':  4.2\n",
    "}\n",
    "charge = { # Simplified charge for H, assuming neutral pH for general calculation\n",
    "    'A':  0, 'R':  1, 'N':  0, 'D': -1, 'C':  0,\n",
    "    'Q':  0, 'E': -1, 'G':  0, 'H':  0, 'I':  0, \n",
    "    'L':  0, 'K':  1, 'M':  0, 'F':  0, 'P':  0,\n",
    "    'S':  0, 'T':  0, 'W':  0, 'Y':  0, 'V':  0\n",
    "}\n",
    "h_donors = {'A':0,'R':2,'N':2,'D':0,'C':0,'Q':2,'E':0,'G':0,'H':1,'I':0,\n",
    "            'L':0,'K':1,'M':0,'F':0,'P':0,'S':1,'T':1,'W':1,'Y':1,'V':0}\n",
    "h_acceptors = {'A':0,'R':0,'N':2,'D':2,'C':1,'Q':2,'E':2,'G':0,'H':1,'I':0,\n",
    "               'L':0,'K':0,'M':0,'F':0,'P':0,'S':1,'T':1,'W':0,'Y':1,'V':0}\n",
    "flexibility = {\n",
    "    'A': 0.357, 'R': 0.529, 'N': 0.463, 'D': 0.511, 'C': 0.346,\n",
    "    'Q': 0.493, 'E': 0.497, 'G': 0.544, 'H': 0.323, 'I': 0.462,\n",
    "    'L': 0.365, 'K': 0.466, 'M': 0.295, 'F': 0.314, 'P': 0.509,\n",
    "    'S': 0.507, 'T': 0.444, 'W': 0.305, 'Y': 0.420, 'V': 0.386\n",
    "}\n",
    "sidechain_volume = {\n",
    "    'A':  88.6, 'R': 173.4, 'N': 114.1, 'D': 111.1, 'C': 108.5, 'Q': 143.8, \n",
    "    'E': 138.4, 'G':  60.1, 'H': 153.2, 'I': 166.7, 'L': 166.7, 'K': 168.6, \n",
    "    'M': 162.9, 'F': 189.9, 'P': 112.7, 'S':  89.0, 'T': 116.1, 'W': 227.8, \n",
    "    'Y': 193.6, 'V': 140.0\n",
    "}\n",
    "polarity = {\n",
    "    'A':  8.1, 'R': 10.5, 'N': 11.6, 'D': 13.0, 'C':  5.5, 'Q': 10.5, \n",
    "    'E': 12.3, 'G':  9.0, 'H': 10.4, 'I':  5.2, 'L':  4.9, 'K': 11.3, \n",
    "    'M':  5.7, 'F':  5.2, 'P':  8.0, 'S':  9.2, 'T':  8.6, 'W':  5.4, \n",
    "    'Y':  6.2, 'V':  5.9\n",
    "}\n",
    "choufa_helix = {\n",
    "    'A': 1.45, 'R': 0.79, 'N': 0.73, 'D': 1.01, 'C': 0.77, 'Q': 1.17, \n",
    "    'E': 1.51, 'G': 0.53, 'H': 1.00, 'I': 1.08, 'L': 1.34, 'K': 1.07, \n",
    "    'M': 1.20, 'F': 1.12, 'P': 0.59, 'S': 0.79, 'T': 0.82, 'W': 1.14, \n",
    "    'Y': 0.61, 'V': 1.06\n",
    "}\n",
    "choufa_sheet = {\n",
    "    'A': 0.97, 'R': 0.90, 'N': 0.65, 'D': 0.54, 'C': 1.30, 'Q': 1.23, \n",
    "    'E': 0.37, 'G': 0.75, 'H': 0.87, 'I': 1.60, 'L': 1.22, 'K': 0.74, \n",
    "    'M': 1.67, 'F': 1.28, 'P': 0.62, 'S': 0.72, 'T': 1.20, 'W': 1.19, \n",
    "    'Y': 1.29, 'V': 1.70\n",
    "}\n",
    "rel_ASA = {\n",
    "    'A': 0.74, 'R': 1.48, 'N': 1.14, 'D': 1.23, 'C': 0.86, 'Q': 1.36, \n",
    "    'E': 1.26, 'G': 1.00, 'H': 0.91, 'I': 0.59, 'L': 0.61, 'K': 1.29, \n",
    "    'M': 0.64, 'F': 0.65, 'P': 0.71, 'S': 1.42, 'T': 1.20, 'W': 0.55, \n",
    "    'Y': 0.63, 'V': 0.54\n",
    "}\n",
    "beta_branched = {aa: (1 if aa in ('V','I','T') else 0) for aa in kd_hydro.keys()}\n",
    "\n",
    "aa_properties_base = {} \n",
    "canonical_set = set(kd_hydro.keys())\n",
    "for aa in canonical_set:\n",
    "    aa_properties_base[aa] = {\n",
    "        'hydro_norm': (kd_hydro[aa] + 4.5) / 9.0,\n",
    "        'charge_val': charge[aa], \n",
    "        'h_donors': h_donors[aa],\n",
    "        'h_acceptors': h_acceptors[aa],\n",
    "        'flexibility': flexibility[aa],\n",
    "        'volume_norm': sidechain_volume[aa] / 227.8,\n",
    "        'pol_norm': (polarity[aa] - 4.9) / (13.0 - 4.9),\n",
    "        'is_aromatic': 1 if aa in ('F','Y','W') else 0,\n",
    "        'helix_prop': choufa_helix[aa] / 1.51,\n",
    "        'sheet_prop': choufa_sheet[aa] / 1.70,\n",
    "        'asa_norm': (rel_ASA[aa] - 0.54) / (1.48 - 0.54),\n",
    "        'is_beta_branched': beta_branched[aa]\n",
    "    }\n",
    "\n",
    "# ─── (B) Load FASTA sequences & Store Raw Sequences with Labels ────────────────\n",
    "def load_fasta_with_labels(filepath, label, filter_non_canonical=False):\n",
    "    sequences_with_labels = []\n",
    "    try:\n",
    "        with open(filepath) as f:\n",
    "            header = None; seq_content = \"\"\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith(\">\"):\n",
    "                    if header is not None and seq_content:\n",
    "                        if (not filter_non_canonical) or (set(seq_content) <= canonical_set):\n",
    "                            sequences_with_labels.append({'sequence': seq_content, 'label': label, 'header': header})\n",
    "                    header = line; seq_content = \"\"\n",
    "                else: seq_content += line\n",
    "            if header is not None and seq_content: # Last sequence\n",
    "                if (not filter_non_canonical) or (set(seq_content) <= canonical_set):\n",
    "                    sequences_with_labels.append({'sequence': seq_content, 'label': label, 'header': header})\n",
    "    except FileNotFoundError: print(f\"Warning: File not found {filepath}.\")\n",
    "    return sequences_with_labels\n",
    "\n",
    "all_sequences_data = []\n",
    "all_sequences_data.extend(load_fasta_with_labels(\"pdb_chains.fasta\", 1))\n",
    "all_sequences_data.extend(load_fasta_with_labels(\"disprot_13000.fasta\", 0))\n",
    "\n",
    "print(f\"Loaded {len([item for item in all_sequences_data if item['label'] == 1])} PDB sequences.\")\n",
    "print(f\"Loaded {len([item for item in all_sequences_data if item['label'] == 0])} DisProt sequences.\")\n",
    "\n",
    "if not all_sequences_data:\n",
    "    print(\"Error: No sequences loaded. Exiting.\"); exit()\n",
    "\n",
    "# ─── (C) NEW Feature Computation Functions ───────────────────────────────────\n",
    "def get_aa_composition(sequence_str):\n",
    "    composition = {aa: 0 for aa in canonical_set}\n",
    "    valid_len = 0\n",
    "    for aa in sequence_str:\n",
    "        if aa in canonical_set:\n",
    "            composition[aa] += 1\n",
    "            valid_len += 1\n",
    "    if valid_len == 0: return {aa: 0.0 for aa in canonical_set}, 0\n",
    "    for aa in composition: composition[aa] /= valid_len\n",
    "    return composition, valid_len\n",
    "\n",
    "def calculate_shannon_entropy(aa_composition):\n",
    "    entropy = 0.0\n",
    "    for aa_freq in aa_composition.values(): # Iterate over frequencies directly\n",
    "        if aa_freq > 0:\n",
    "            entropy -= aa_freq * math.log2(aa_freq)\n",
    "    return entropy\n",
    "\n",
    "def compute_new_seven_features(sequence_str):\n",
    "    if not sequence_str: return np.zeros(7)\n",
    "    composition, valid_seq_len = get_aa_composition(sequence_str)\n",
    "    if valid_seq_len == 0: return np.zeros(7)\n",
    "\n",
    "    hydro_norm_sum, flex_norm_sum, h_bond_potential_sum = 0, 0, 0\n",
    "    for aa in sequence_str:\n",
    "        if aa in aa_properties_base:\n",
    "            props = aa_properties_base[aa]\n",
    "            hydro_norm_sum += props['hydro_norm']\n",
    "            flex_norm_sum += props['flexibility'] / 0.544\n",
    "            h_bond_potential_sum += props['h_donors'] + props['h_acceptors']\n",
    "\n",
    "    net_charge_prop = (composition.get('R',0) + composition.get('K',0)) - \\\n",
    "                      (composition.get('D',0) + composition.get('E',0))\n",
    "    bulky_hydrophobics_list = ['W', 'C', 'F', 'Y', 'I', 'V', 'L']\n",
    "    \n",
    "    return np.array([\n",
    "        hydro_norm_sum / valid_seq_len,\n",
    "        flex_norm_sum / valid_seq_len,\n",
    "        h_bond_potential_sum / valid_seq_len,\n",
    "        abs(net_charge_prop),\n",
    "        calculate_shannon_entropy(composition),\n",
    "        composition.get('P', 0),\n",
    "        sum(composition.get(aa, 0) for aa in bulky_hydrophobics_list)\n",
    "    ])\n",
    "\n",
    "def compute_features_for_dataset(sequence_list, window_size_param=None):\n",
    "    \"\"\"\n",
    "    Computes the new 7 features for a list of sequence strings.\n",
    "    If window_size_param is None, computes global features.\n",
    "    If window_size_param is an int, computes features for each window and averages them.\n",
    "    \"\"\"\n",
    "    all_feature_vectors = []\n",
    "    for seq_str in sequence_list:\n",
    "        if not seq_str: \n",
    "            all_feature_vectors.append(np.zeros(7))\n",
    "            continue\n",
    "        \n",
    "        canonical_sequence = \"\".join([aa for aa in seq_str if aa in canonical_set])\n",
    "        if not canonical_sequence: \n",
    "            all_feature_vectors.append(np.zeros(7))\n",
    "            continue\n",
    "\n",
    "        if window_size_param is None or len(canonical_sequence) < window_size_param:\n",
    "            all_feature_vectors.append(compute_new_seven_features(canonical_sequence))\n",
    "        else:\n",
    "            window_derived_feature_sets = [] \n",
    "            for i in range(len(canonical_sequence) - window_size_param + 1):\n",
    "                window_segment_str = canonical_sequence[i : i + window_size_param]\n",
    "                window_features = compute_new_seven_features(window_segment_str)\n",
    "                window_derived_feature_sets.append(window_features)\n",
    "            if not window_derived_feature_sets:\n",
    "                all_feature_vectors.append(compute_new_seven_features(canonical_sequence)) # Fallback\n",
    "            else:\n",
    "                all_feature_vectors.append(np.mean(np.vstack(window_derived_feature_sets), axis=0))\n",
    "    return all_feature_vectors\n",
    "\n",
    "# --- Prepare data for Global New Features Classifier ---\n",
    "new_feature_names = [\n",
    "    \"hydro_norm_avg\", \"flex_norm_avg\", \"h_bond_potential_avg\",\n",
    "    \"abs_net_charge_prop\", \"shannon_entropy\", \"freq_proline\", \"freq_bulky_hydrophobics\"\n",
    "]\n",
    "print(f\"\\nComputing NEW GLOBAL features (WINDOW_SIZE_GLOBAL_FEATURES = {WINDOW_SIZE_GLOBAL_FEATURES})...\")\n",
    "raw_sequences_list = [item['sequence'] for item in all_sequences_data]\n",
    "labels_list = [item['label'] for item in all_sequences_data]\n",
    "\n",
    "global_features_calculated = compute_features_for_dataset(raw_sequences_list, window_size_param=WINDOW_SIZE_GLOBAL_FEATURES)\n",
    "\n",
    "df_global_features = pd.DataFrame(global_features_calculated, columns=new_feature_names)\n",
    "df_global_features[\"label\"] = labels_list\n",
    "print(\"NEW GLOBAL feature computation complete.\")\n",
    "\n",
    "if df_global_features.empty or df_global_features['label'].nunique() < 2:\n",
    "    print(\"Error: Not enough data for global features. Exiting.\"); exit()\n",
    "    \n",
    "X_global_train, X_global_test, y_global_train, y_global_test, train_indices_global, test_indices_global = train_test_split(\n",
    "    df_global_features.drop(columns=[\"label\"]),\n",
    "    df_global_features[\"label\"],\n",
    "    np.arange(len(raw_sequences_list)), \n",
    "    test_size=0.2, random_state=42, stratify=df_global_features[\"label\"] \n",
    ")\n",
    "\n",
    "df_train_global_features = X_global_train.copy()\n",
    "df_train_global_features[\"label\"] = y_global_train\n",
    "\n",
    "# Raw sequences for the test set (will be used by the sliding window classifier)\n",
    "test_raw_sequences_for_sliding_window = [raw_sequences_list[i] for i in test_indices_global]\n",
    "y_test_for_sliding_window = y_global_test # True labels for the test set\n",
    "\n",
    "print(f\"\\nGlobal Features Training set size: {len(df_train_global_features)}\")\n",
    "print(f\"Global Features Testing set size: {len(X_global_test)}\")\n",
    "\n",
    "# --- Calculate Midpoints from Global New Features Training Data ---\n",
    "if df_train_global_features[\"label\"].nunique() < 2:\n",
    "    print(\"\\nError: Global features training set lacks class diversity for midpoints.\"); exit()\n",
    "else:\n",
    "    train_means_global = df_train_global_features.groupby(\"label\").mean().rename(index={0:\"DisProt\", 1:\"PDB\"})\n",
    "    if \"PDB\" not in train_means_global.index or \"DisProt\" not in train_means_global.index:\n",
    "        print(\"\\nError: Could not find means for both PDB and DisProt in global training data.\"); exit()\n",
    "    else:\n",
    "        midpoints_global_new_features = {col: (train_means_global.loc[\"PDB\", col] + train_means_global.loc[\"DisProt\", col]) / 2\n",
    "                                         for col in X_global_train.columns}\n",
    "        print(\"\\nGlobal Feature Means (DisProt vs. PDB) from NEW GLOBAL FEATURES TRAINING DATA:\\n\")\n",
    "        print(train_means_global, \"\\n\")\n",
    "        print(\"Chosen Midpoint Thresholds (from NEW GLOBAL FEATURES TRAINING DATA):\\n\")\n",
    "        for feat, t in midpoints_global_new_features.items(): print(f\"  {feat:18s} = {t:.3f}\")\n",
    "        print()\n",
    "\n",
    "# --- Helper to count conditions met for the NEW 7 features ---\n",
    "def count_conditions_for_new_feature_vector(new_feature_vector_values, midpoints_dict, train_means_for_direction):\n",
    "    row = pd.Series(new_feature_vector_values, index=new_feature_names)\n",
    "    conditions_met_count = 0\n",
    "    \n",
    "    # hydro_norm_avg: PDB typically higher\n",
    "    if row[\"hydro_norm_avg\"] >= midpoints_dict.get(\"hydro_norm_avg\", 0.0): conditions_met_count +=1\n",
    "    # flex_norm_avg: PDB typically lower\n",
    "    if row[\"flex_norm_avg\"] <= midpoints_dict.get(\"flex_norm_avg\", float('inf')): conditions_met_count +=1\n",
    "    # h_bond_potential_avg: PDB typically lower\n",
    "    if row[\"h_bond_potential_avg\"] <= midpoints_dict.get(\"h_bond_potential_avg\", float('inf')): conditions_met_count +=1\n",
    "    # abs_net_charge_prop: PDB typically lower\n",
    "    if row[\"abs_net_charge_prop\"] <= midpoints_dict.get(\"abs_net_charge_prop\", float('inf')): conditions_met_count +=1\n",
    "    # shannon_entropy: PDB typically higher (inspect means to confirm this assumption)\n",
    "    if train_means_for_direction.loc[\"PDB\", \"shannon_entropy\"] > train_means_for_direction.loc[\"DisProt\", \"shannon_entropy\"]:\n",
    "        if row[\"shannon_entropy\"] >= midpoints_dict.get(\"shannon_entropy\", 0.0): conditions_met_count +=1\n",
    "    else: # PDB shannon_entropy is lower or equal\n",
    "        if row[\"shannon_entropy\"] <= midpoints_dict.get(\"shannon_entropy\", float('inf')): conditions_met_count +=1\n",
    "    # freq_proline: PDB typically lower\n",
    "    if row[\"freq_proline\"] <= midpoints_dict.get(\"freq_proline\", float('inf')): conditions_met_count +=1\n",
    "    # freq_bulky_hydrophobics: PDB typically higher\n",
    "    if row[\"freq_bulky_hydrophobics\"] >= midpoints_dict.get(\"freq_bulky_hydrophobics\", 0.0): conditions_met_count +=1\n",
    "    \n",
    "    return conditions_met_count\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# --- 1. New Global Features Threshold-Based Classifier Evaluation ---\n",
    "# ----------------------------------------------------------------------------------\n",
    "print(\"\\n\\n--- Evaluating New Global Features Threshold-Based Classifier ---\")\n",
    "df_test_global_features_eval = X_global_test.copy()\n",
    "df_test_global_features_eval[\"label\"] = y_global_test\n",
    "\n",
    "if df_test_global_features_eval.empty:\n",
    "    print(\"Global features test set is empty. Skipping evaluation.\")\n",
    "else:\n",
    "    df_test_global_features_eval[\"conditions_met\"] = df_test_global_features_eval.apply(\n",
    "        lambda r: count_conditions_for_new_feature_vector(r[new_feature_names].values, midpoints_global_new_features, train_means_global), axis=1\n",
    "    )\n",
    "    \n",
    "    dist_test_global = df_test_global_features_eval.groupby(\"label\")[\"conditions_met\"].value_counts().unstack(fill_value=0).rename(index={0:\"DisProt\", 1:\"PDB\"})\n",
    "    print(\"Distribution of ‘conditions_met’ (NEW GLOBAL features) by Label (ON TEST SET):\\n\")\n",
    "    print(dist_test_global, \"\\n\")\n",
    "\n",
    "    results_global = []\n",
    "    for k_thresh in range(1, 8):\n",
    "        preds_test_global = (df_test_global_features_eval[\"conditions_met\"] >= k_thresh).astype(int)\n",
    "        tp = ((preds_test_global == 1) & (y_global_test == 1)).sum()\n",
    "        fn = ((preds_test_global == 0) & (y_global_test == 1)).sum()\n",
    "        tn = ((preds_test_global == 0) & (y_global_test == 0)).sum()\n",
    "        fp = ((preds_test_global == 1) & (y_global_test == 0)).sum()\n",
    "        acc = (tp + tn) / len(y_global_test) if len(y_global_test) > 0 else 0\n",
    "        prec_pdb = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        rec_pdb = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_pdb = 2*(prec_pdb*rec_pdb)/(prec_pdb+rec_pdb) if (prec_pdb+rec_pdb)>0 else 0\n",
    "        results_global.append({\n",
    "            \"k\": k_thresh, \"TP\": tp, \"FN\": fn, \"TN\": tn, \"FP\": fp, \"Accuracy\": f\"{acc:.2%}\",\n",
    "            \"Precision (PDB)\": f\"{prec_pdb:.2%}\", \"Recall (PDB)\": f\"{rec_pdb:.2%}\", \"F1-score (PDB)\": f\"{f1_pdb:.2%}\"\n",
    "        })\n",
    "    df_results_global = pd.DataFrame(results_global)\n",
    "    print(\"Performance of New Global Features Classifier on TEST SET (varying k):\\n\")\n",
    "    print(df_results_global.to_string(index=False))\n",
    "\n",
    "    if not df_results_global.empty:\n",
    "        try:\n",
    "            df_results_global['F1_PDB_float'] = df_results_global['F1-score (PDB)'].str.rstrip('%').astype('float') / 100.0\n",
    "            best_k_row_global = df_results_global.loc[df_results_global['F1_PDB_float'].idxmax()]\n",
    "            best_k_global = int(best_k_row_global[\"k\"])\n",
    "            print(f\"\\n--- Detailed New Global Features Classification Report for best k = {best_k_global} (based on F1 PDB) ---\")\n",
    "            best_preds_global = (df_test_global_features_eval[\"conditions_met\"] >= best_k_global).astype(int)\n",
    "            print(classification_report(y_global_test, best_preds_global, target_names=[\"DisProt (0)\", \"PDB (1)\"], zero_division=0))\n",
    "            cm_global = confusion_matrix(y_global_test, best_preds_global)\n",
    "            print(\"Confusion Matrix for best k (New Global Features):\\n\", pd.DataFrame(cm_global, index=[\"Actual DisProt\",\"Actual PDB\"], columns=[\"Pred DisProt\",\"Pred PDB\"]))\n",
    "        except Exception as e: print(f\"Error in detailed report for global features: {e}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# --- 2. Sliding Window (Larger) Classifier with Failure Cancellation ---\n",
    "# ----------------------------------------------------------------------------------\n",
    "print(\"\\n\\n--- Testing Sliding Window (Larger) Classifier with Failure Cancellation ---\")\n",
    "\n",
    "def classify_protein_sliding_window_cancellation_new_features(\n",
    "    sequence_str, window_size, slide_step,\n",
    "    midpoints_for_eval, window_k_pass_thresh, \n",
    "    max_allowed_total_failures, train_means_for_direction_check): # Added train_means\n",
    "\n",
    "    if not sequence_str: return 1 \n",
    "    canonical_sequence = \"\".join([aa for aa in sequence_str if aa in canonical_set])\n",
    "    if not canonical_sequence or len(canonical_sequence) < window_size: return 1 \n",
    "\n",
    "    current_consecutive_failures_streak = 0\n",
    "    num_windows_processed = 0\n",
    "\n",
    "    for i in range(0, len(canonical_sequence) - window_size + 1, slide_step):\n",
    "        window_str = canonical_sequence[i : i + window_size]\n",
    "        num_windows_processed += 1\n",
    "        \n",
    "        seven_new_features_for_current_window = compute_new_seven_features(window_str)\n",
    "        num_conditions_this_window_met = count_conditions_for_new_feature_vector(\n",
    "            seven_new_features_for_current_window, \n",
    "            midpoints_for_eval,\n",
    "            train_means_for_direction_check # Pass train_means here\n",
    "        )\n",
    "        \n",
    "        window_passes = (num_conditions_this_window_met >= window_k_pass_thresh)\n",
    "        \n",
    "        if window_passes: current_consecutive_failures_streak = 0 \n",
    "        else: current_consecutive_failures_streak += 1\n",
    "            \n",
    "    if num_windows_processed == 0: return 0 \n",
    "    total_unforgiven_failures = current_consecutive_failures_streak\n",
    "    \n",
    "    return 1 if total_unforgiven_failures <= max_allowed_total_failures else 0\n",
    "\n",
    "print(f\"\\nApplying Sliding Window (Larger) Classifier (NEW features) with Failure Cancellation: window_size={SLIDING_WINDOW_SIZE}, slide_step={SLIDING_WINDOW_SLIDE_STEP}, window_k_pass_thresh={SLIDING_WINDOW_PASS_K}, max_total_unforgiven_failures={MAX_UNFORGIVEN_FAILED_WINDOWS_SLIDING}...\")\n",
    "predictions_sliding_window_test = []\n",
    "if not test_raw_sequences_for_sliding_window:\n",
    "    print(\"No raw sequences in test set for sliding window classifier.\")\n",
    "else:\n",
    "    for raw_seq in test_raw_sequences_for_sliding_window:\n",
    "        pred = classify_protein_sliding_window_cancellation_new_features(\n",
    "            raw_seq, SLIDING_WINDOW_SIZE, SLIDING_WINDOW_SLIDE_STEP,\n",
    "            midpoints_global_new_features, # Use midpoints from global new features training\n",
    "            SLIDING_WINDOW_PASS_K,\n",
    "            MAX_UNFORGIVEN_FAILED_WINDOWS_SLIDING,\n",
    "            train_means_global # Pass train_means for direction check\n",
    "        )\n",
    "        predictions_sliding_window_test.append(pred)\n",
    "    print(\"\\nSliding Window (Larger) with Failure Cancellation classification complete.\")\n",
    "\n",
    "    if predictions_sliding_window_test:\n",
    "        print(\"\\nPerformance of Sliding Window (Larger) Classifier with Failure Cancellation (ON TEST SET):\\n\")\n",
    "        print(classification_report(y_test_for_sliding_window, predictions_sliding_window_test, target_names=[\"DisProt (0)\", \"PDB (1)\"], zero_division=0))\n",
    "        cm_sliding = confusion_matrix(y_test_for_sliding_window, predictions_sliding_window_test)\n",
    "        print(\"Confusion Matrix:\\n\", pd.DataFrame(cm_sliding, index=[\"Actual DisProt\",\"Actual PDB\"], columns=[\"Pred DisProt\",\"Pred PDB\"]))\n",
    "        acc_sliding = (cm_sliding[0,0] + cm_sliding[1,1]) / np.sum(cm_sliding) if np.sum(cm_sliding) > 0 else 0\n",
    "        print(f\"Accuracy: {acc_sliding:.2%}\")\n",
    "    else:\n",
    "        print(\"No predictions made by Sliding Window (Larger) classifier.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
