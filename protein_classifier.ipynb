{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b534985e-b516-41af-b343-14da11c248bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2714 Extracted 15000 chains \u2192 'pdb_chains.fasta'\n"
     ]
    }
   ],
   "source": [
    "# 1.) Download the PDB chain sequences (FASTA format from RCSB) via the HTTPS mirror,\n",
    "#      then keep only the first 15 000 entries.\n",
    "\n",
    "import requests\n",
    "\n",
    "# Use the \u201cfiles.wwpdb.org\u201d HTTPS mirror instead of FTP\n",
    "pdb_url = \"https://files.wwpdb.org/pub/pdb/derived_data/pdb_seqres.txt\"\n",
    "try:\n",
    "    resp = requests.get(pdb_url, timeout=60)\n",
    "    resp.raise_for_status()\n",
    "    text = resp.text.strip()\n",
    "    if not text.startswith(\">\"):\n",
    "        raise RuntimeError(\"Downloaded content does not look like FASTA.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to download PDB chain sequences: {e}\")\n",
    "\n",
    "# Write the complete dump to a temporary file\n",
    "with open(\"pdb_chains.fasta\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text + \"\\n\")\n",
    "\n",
    "# \u2500\u2500\u2500 Split the full FASTA into individual (header, sequence) tuples \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def split_fasta(filepath):\n",
    "    sequences = []\n",
    "    with open(filepath, \"r\") as f:\n",
    "        header = None\n",
    "        seq_lines = []\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if line.startswith(\">\"):\n",
    "                if header is not None:\n",
    "                    sequences.append((header, \"\".join(seq_lines)))\n",
    "                header = line\n",
    "                seq_lines = []\n",
    "            else:\n",
    "                seq_lines.append(line)\n",
    "        # Add the final sequence\n",
    "        if header is not None:\n",
    "            sequences.append((header, \"\".join(seq_lines)))\n",
    "    return sequences\n",
    "\n",
    "all_chains = split_fasta(\"pdb_chains.fasta\")\n",
    "\n",
    "# \u2500\u2500\u2500 Keep exactly the first 15 000 chains \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "subset = all_chains[:15000]\n",
    "\n",
    "# \u2500\u2500\u2500 Write those 15 000 chains back to \u201cpdb_chains.fasta\u201d \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "with open(\"pdb_chains.fasta\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for header, seq in subset:\n",
    "        f.write(f\"{header}\\n\")\n",
    "        f.write(f\"{seq}\\n\")\n",
    "\n",
    "print(f\"\u2714 Extracted {len(subset)} chains \u2192 'pdb_chains.fasta'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e30de29-e8f5-4fe7-abfd-58594dc600e6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2714 Successfully fetched 100 DisProt sequences in FASTA format \u2192 'disprot_1000.fasta'\n"
     ]
    }
   ],
   "source": [
    "# 2.) Use DisProt\u2019s search endpoint with format=fasta\n",
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"https://disprot.org/api/search?format=fasta&limit=10000\"\n",
    "try:\n",
    "    resp = requests.get(url, timeout=15)\n",
    "    resp.raise_for_status()\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to GET DisProt FASTA via API: {e}\")\n",
    "\n",
    "text = resp.text.strip()\n",
    "\n",
    "# 2.2) Quick sanity check: FASTA must start with '>', not '<'\n",
    "if not text.startswith(\">\"):\n",
    "    raise RuntimeError(\n",
    "        \"Downloaded content does not look like FASTA. \"\n",
    "        \"If it begins with '<', you're still hitting an HTML page instead of raw FASTA.\"\n",
    "    )\n",
    "\n",
    "# 2.3) Write the 100 DisProt entries to a file\n",
    "with open(\"disprot_13000.fasta\", \"w\") as f:\n",
    "    f.write(text + \"\\n\")\n",
    "\n",
    "print(\"\u2714 Successfully fetched 100 DisProt sequences in FASTA format \u2192 'disprot_1000.fasta'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b9b660b-dd1e-48ac-a9d2-8dc830cae3d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2714 Fetched 25000 DisProt sequences \u2192 'disprot_13000.fasta'\n"
     ]
    }
   ],
   "source": [
    "# 2.1) Collect more data\n",
    "\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# \u2500\u2500\u2500 PARAMETERS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "TOTAL_DESIRED = 25_000   # how many DisProt sequences we want total\n",
    "PER_PAGE      = 100      # DisProt\u2019s hard cap per request\n",
    "OUTPUT_FILE   = \"disprot_13000.fasta\"\n",
    "\n",
    "accum_seqs = []\n",
    "offset     = 0\n",
    "\n",
    "while len(accum_seqs) < TOTAL_DESIRED:\n",
    "    url = f\"https://disprot.org/api/search?format=fasta&limit={PER_PAGE}&offset={offset}\"\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to GET DisProt FASTA (offset={offset}): {e}\")\n",
    "\n",
    "    block = resp.text.strip()\n",
    "    if not block.startswith(\">\"):\n",
    "        raise RuntimeError(\n",
    "            \"Downloaded content does not look like FASTA. \"\n",
    "            \"If it begins with '<', you're still hitting an HTML page.\"\n",
    "        )\n",
    "\n",
    "    # Parse out this page\u2019s FASTA sequences (collecting only the raw sequences, not full headers):\n",
    "    raw_lines = block.splitlines()\n",
    "    header = None\n",
    "    seq_buf = \"\"\n",
    "    this_page_seqs = []\n",
    "    for line in raw_lines:\n",
    "        if line.startswith(\">\"):\n",
    "            if header is not None and seq_buf:\n",
    "                this_page_seqs.append(seq_buf)\n",
    "            header = line\n",
    "            seq_buf = \"\"\n",
    "        else:\n",
    "            seq_buf += line.strip()\n",
    "    if header is not None and seq_buf:\n",
    "        this_page_seqs.append(seq_buf)\n",
    "\n",
    "    if not this_page_seqs:\n",
    "        # No more sequences returned \u2192 break out early\n",
    "        break\n",
    "\n",
    "    accum_seqs.extend(this_page_seqs)\n",
    "    offset += PER_PAGE\n",
    "\n",
    "    # Sleep briefly (so we don\u2019t hammer the server)\n",
    "    time.sleep(0.4)\n",
    "\n",
    "# Trim in case we overshot\n",
    "accum_seqs = accum_seqs[:TOTAL_DESIRED]\n",
    "\n",
    "# Write out ~25k sequences in FASTA format (with minimal headers)\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    for i, seq in enumerate(accum_seqs):\n",
    "        f.write(f\">disprot_sequence_{i+1}\\n\")\n",
    "        f.write(seq + \"\\n\")\n",
    "\n",
    "print(f\"\u2714 Fetched {len(accum_seqs)} DisProt sequences \u2192 '{OUTPUT_FILE}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4177fdf8-80da-43a8-8fa1-2633860ed0cb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">disprot_sequence_1\n",
      "EHVIEMDVTSENGQRALKEQSSKAKIVKNRWGRNVVQISNT\n",
      ">disprot_sequence_2\n",
      "VYRNSRAQGGG\n",
      ">disprot_sequence_3\n"
     ]
    }
   ],
   "source": [
    "# 2.2) Verify Downloaded Sequences\n",
    "with open(\"disprot_13000.fasta\") as f:\n",
    "    for _ in range(5):\n",
    "        print(f.readline().rstrip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adffd98f-0799-4b9e-9883-c790a96efe60",
   "metadata": {},
   "source": [
    "# Sliding Window vs Global Learned Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a75ce8de-d866-4859-861f-81bc69fef95e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15000 PDB sequences.\n",
      "Loaded 25000 DisProt sequences.\n",
      "\n",
      "Computing NEW GLOBAL features (WINDOW_SIZE_GLOBAL_FEATURES = None)...\n",
      "NEW GLOBAL feature computation complete.\n",
      "\n",
      "Global Features Training set size: 32000\n",
      "Global Features Testing set size: 8000\n",
      "\n",
      "Global Feature Means (DisProt vs. PDB) from NEW GLOBAL FEATURES TRAINING DATA:\n",
      "\n",
      "         hydro_norm_avg  flex_norm_avg  h_bond_potential_avg  \\\n",
      "label                                                          \n",
      "DisProt        0.401099       0.837249              1.256680   \n",
      "PDB            0.475068       0.806553              1.073287   \n",
      "\n",
      "         abs_net_charge_prop  shannon_entropy  freq_proline  \\\n",
      "label                                                         \n",
      "DisProt             0.115245         3.359850      0.069069   \n",
      "PDB                 0.036236         3.700737      0.043093   \n",
      "\n",
      "         freq_bulky_hydrophobics  \n",
      "label                             \n",
      "DisProt                 0.210505  \n",
      "PDB                     0.312975   \n",
      "\n",
      "Chosen Midpoint Thresholds (from NEW GLOBAL FEATURES TRAINING DATA):\n",
      "\n",
      "  hydro_norm_avg     = 0.438\n",
      "  flex_norm_avg      = 0.822\n",
      "  h_bond_potential_avg = 1.165\n",
      "  abs_net_charge_prop = 0.076\n",
      "  shannon_entropy    = 3.530\n",
      "  freq_proline       = 0.056\n",
      "  freq_bulky_hydrophobics = 0.262\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluating New Global Features Threshold-Based Classifier ---\n",
      "Distribution of \u2018conditions_met\u2019 (NEW GLOBAL features) by Label (ON TEST SET):\n",
      "\n",
      "conditions_met    0     1     2     3    4    5     6    7\n",
      "label                                                     \n",
      "DisProt         299  1014  1205  1034  716  469   209   54\n",
      "PDB               6    28    49    90  333  563  1074  857 \n",
      "\n",
      "Performance of New Global Features Classifier on TEST SET (varying k):\n",
      "\n",
      " k   TP   FN   TN   FP Accuracy Precision (PDB) Recall (PDB) F1-score (PDB)\n",
      " 1 2994    6  299 4701   41.16%          38.91%       99.80%         55.99%\n",
      " 2 2966   34 1313 3687   53.49%          44.58%       98.87%         61.45%\n",
      " 3 2917   83 2518 2482   67.94%          54.03%       97.23%         69.46%\n",
      " 4 2827  173 3552 1448   79.74%          66.13%       94.23%         77.72%\n",
      " 5 2494  506 4268  732   84.52%          77.31%       83.13%         80.12%\n",
      " 6 1931 1069 4737  263   83.35%          88.01%       64.37%         74.36%\n",
      " 7  857 2143 4946   54   72.54%          94.07%       28.57%         43.83%\n",
      "\n",
      "--- Detailed New Global Features Classification Report for best k = 5 (based on F1 PDB) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " DisProt (0)       0.89      0.85      0.87      5000\n",
      "     PDB (1)       0.77      0.83      0.80      3000\n",
      "\n",
      "    accuracy                           0.85      8000\n",
      "   macro avg       0.83      0.84      0.84      8000\n",
      "weighted avg       0.85      0.85      0.85      8000\n",
      "\n",
      "Confusion Matrix for best k (New Global Features):\n",
      "                 Pred DisProt  Pred PDB\n",
      "Actual DisProt          4268       732\n",
      "Actual PDB               506      2494\n",
      "\n",
      "\n",
      "--- Testing Sliding Window (Larger) Classifier with Failure Cancellation ---\n",
      "\n",
      "Applying Sliding Window (Larger) Classifier (NEW features) with Failure Cancellation: window_size=9, slide_step=9, window_k_pass_thresh=4, max_total_unforgiven_failures=3...\n",
      "\n",
      "Sliding Window (Larger) with Failure Cancellation classification complete.\n",
      "\n",
      "Performance of Sliding Window (Larger) Classifier with Failure Cancellation (ON TEST SET):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " DisProt (0)       0.86      0.18      0.30      5000\n",
      "     PDB (1)       0.41      0.95      0.57      3000\n",
      "\n",
      "    accuracy                           0.47      8000\n",
      "   macro avg       0.64      0.57      0.44      8000\n",
      "weighted avg       0.69      0.47      0.40      8000\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Pred DisProt  Pred PDB\n",
      "Actual DisProt           910      4090\n",
      "Actual PDB               149      2851\n",
      "Accuracy: 47.01%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "\n",
    "# --- Parameters for Classifiers ---\n",
    "# For Global Feature Classifier (WINDOW_SIZE_GLOBAL_FEATURES = None means direct global calculation)\n",
    "WINDOW_SIZE_GLOBAL_FEATURES = None \n",
    "\n",
    "# For Sliding Window Classifier\n",
    "SLIDING_WINDOW_SIZE = 9  # Size of the sliding window\n",
    "SLIDING_WINDOW_SLIDE_STEP = 9 # Step for sliding (equal to WINDOW_SIZE for non-overlapping)\n",
    "SLIDING_WINDOW_PASS_K = 5     # Min conditions a window must meet to \"pass\"\n",
    "MAX_UNFORGIVEN_FAILED_WINDOWS_SLIDING = 3 # Max uncancelled failed windows for protein to pass\n",
    "\n",
    "# \u2500\u2500\u2500 (A) Build aa_properties (underlying single AA properties) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "kd_hydro = {\n",
    "    'A':  1.8, 'R': -4.5, 'N': -3.5, 'D': -3.5, 'C':  2.5,\n",
    "    'Q': -3.5, 'E': -3.5, 'G': -0.4, 'H': -3.2, 'I':  4.5,\n",
    "    'L':  3.8, 'K': -3.9, 'M':  1.9, 'F':  2.8, 'P': -1.6,\n",
    "    'S': -0.8, 'T': -0.7, 'W': -0.9, 'Y': -1.3, 'V':  4.2\n",
    "}\n",
    "charge = { # Simplified charge for H, assuming neutral pH for general calculation\n",
    "    'A':  0, 'R':  1, 'N':  0, 'D': -1, 'C':  0,\n",
    "    'Q':  0, 'E': -1, 'G':  0, 'H':  0, 'I':  0, \n",
    "    'L':  0, 'K':  1, 'M':  0, 'F':  0, 'P':  0,\n",
    "    'S':  0, 'T':  0, 'W':  0, 'Y':  0, 'V':  0\n",
    "}\n",
    "h_donors = {'A':0,'R':2,'N':2,'D':0,'C':0,'Q':2,'E':0,'G':0,'H':1,'I':0,\n",
    "            'L':0,'K':1,'M':0,'F':0,'P':0,'S':1,'T':1,'W':1,'Y':1,'V':0}\n",
    "h_acceptors = {'A':0,'R':0,'N':2,'D':2,'C':1,'Q':2,'E':2,'G':0,'H':1,'I':0,\n",
    "               'L':0,'K':0,'M':0,'F':0,'P':0,'S':1,'T':1,'W':0,'Y':1,'V':0}\n",
    "flexibility = {\n",
    "    'A': 0.357, 'R': 0.529, 'N': 0.463, 'D': 0.511, 'C': 0.346,\n",
    "    'Q': 0.493, 'E': 0.497, 'G': 0.544, 'H': 0.323, 'I': 0.462,\n",
    "    'L': 0.365, 'K': 0.466, 'M': 0.295, 'F': 0.314, 'P': 0.509,\n",
    "    'S': 0.507, 'T': 0.444, 'W': 0.305, 'Y': 0.420, 'V': 0.386\n",
    "}\n",
    "sidechain_volume = {\n",
    "    'A':  88.6, 'R': 173.4, 'N': 114.1, 'D': 111.1, 'C': 108.5, 'Q': 143.8, \n",
    "    'E': 138.4, 'G':  60.1, 'H': 153.2, 'I': 166.7, 'L': 166.7, 'K': 168.6, \n",
    "    'M': 162.9, 'F': 189.9, 'P': 112.7, 'S':  89.0, 'T': 116.1, 'W': 227.8, \n",
    "    'Y': 193.6, 'V': 140.0\n",
    "}\n",
    "polarity = {\n",
    "    'A':  8.1, 'R': 10.5, 'N': 11.6, 'D': 13.0, 'C':  5.5, 'Q': 10.5, \n",
    "    'E': 12.3, 'G':  9.0, 'H': 10.4, 'I':  5.2, 'L':  4.9, 'K': 11.3, \n",
    "    'M':  5.7, 'F':  5.2, 'P':  8.0, 'S':  9.2, 'T':  8.6, 'W':  5.4, \n",
    "    'Y':  6.2, 'V':  5.9\n",
    "}\n",
    "choufa_helix = {\n",
    "    'A': 1.45, 'R': 0.79, 'N': 0.73, 'D': 1.01, 'C': 0.77, 'Q': 1.17, \n",
    "    'E': 1.51, 'G': 0.53, 'H': 1.00, 'I': 1.08, 'L': 1.34, 'K': 1.07, \n",
    "    'M': 1.20, 'F': 1.12, 'P': 0.59, 'S': 0.79, 'T': 0.82, 'W': 1.14, \n",
    "    'Y': 0.61, 'V': 1.06\n",
    "}\n",
    "choufa_sheet = {\n",
    "    'A': 0.97, 'R': 0.90, 'N': 0.65, 'D': 0.54, 'C': 1.30, 'Q': 1.23, \n",
    "    'E': 0.37, 'G': 0.75, 'H': 0.87, 'I': 1.60, 'L': 1.22, 'K': 0.74, \n",
    "    'M': 1.67, 'F': 1.28, 'P': 0.62, 'S': 0.72, 'T': 1.20, 'W': 1.19, \n",
    "    'Y': 1.29, 'V': 1.70\n",
    "}\n",
    "rel_ASA = {\n",
    "    'A': 0.74, 'R': 1.48, 'N': 1.14, 'D': 1.23, 'C': 0.86, 'Q': 1.36, \n",
    "    'E': 1.26, 'G': 1.00, 'H': 0.91, 'I': 0.59, 'L': 0.61, 'K': 1.29, \n",
    "    'M': 0.64, 'F': 0.65, 'P': 0.71, 'S': 1.42, 'T': 1.20, 'W': 0.55, \n",
    "    'Y': 0.63, 'V': 0.54\n",
    "}\n",
    "beta_branched = {aa: (1 if aa in ('V','I','T') else 0) for aa in kd_hydro.keys()}\n",
    "\n",
    "aa_properties_base = {} \n",
    "canonical_set = set(kd_hydro.keys())\n",
    "for aa in canonical_set:\n",
    "    aa_properties_base[aa] = {\n",
    "        'hydro_norm': (kd_hydro[aa] + 4.5) / 9.0,\n",
    "        'charge_val': charge[aa], \n",
    "        'h_donors': h_donors[aa],\n",
    "        'h_acceptors': h_acceptors[aa],\n",
    "        'flexibility': flexibility[aa],\n",
    "        'volume_norm': sidechain_volume[aa] / 227.8,\n",
    "        'pol_norm': (polarity[aa] - 4.9) / (13.0 - 4.9),\n",
    "        'is_aromatic': 1 if aa in ('F','Y','W') else 0,\n",
    "        'helix_prop': choufa_helix[aa] / 1.51,\n",
    "        'sheet_prop': choufa_sheet[aa] / 1.70,\n",
    "        'asa_norm': (rel_ASA[aa] - 0.54) / (1.48 - 0.54),\n",
    "        'is_beta_branched': beta_branched[aa]\n",
    "    }\n",
    "\n",
    "# \u2500\u2500\u2500 (B) Load FASTA sequences & Store Raw Sequences with Labels \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def load_fasta_with_labels(filepath, label, filter_non_canonical=False):\n",
    "    sequences_with_labels = []\n",
    "    try:\n",
    "        with open(filepath) as f:\n",
    "            header = None; seq_content = \"\"\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith(\">\"):\n",
    "                    if header is not None and seq_content:\n",
    "                        if (not filter_non_canonical) or (set(seq_content) <= canonical_set):\n",
    "                            sequences_with_labels.append({'sequence': seq_content, 'label': label, 'header': header})\n",
    "                    header = line; seq_content = \"\"\n",
    "                else: seq_content += line\n",
    "            if header is not None and seq_content: # Last sequence\n",
    "                if (not filter_non_canonical) or (set(seq_content) <= canonical_set):\n",
    "                    sequences_with_labels.append({'sequence': seq_content, 'label': label, 'header': header})\n",
    "    except FileNotFoundError: print(f\"Warning: File not found {filepath}.\")\n",
    "    return sequences_with_labels\n",
    "\n",
    "all_sequences_data = []\n",
    "all_sequences_data.extend(load_fasta_with_labels(\"pdb_chains.fasta\", 1))\n",
    "all_sequences_data.extend(load_fasta_with_labels(\"disprot_13000.fasta\", 0))\n",
    "\n",
    "print(f\"Loaded {len([item for item in all_sequences_data if item['label'] == 1])} PDB sequences.\")\n",
    "print(f\"Loaded {len([item for item in all_sequences_data if item['label'] == 0])} DisProt sequences.\")\n",
    "\n",
    "if not all_sequences_data:\n",
    "    print(\"Error: No sequences loaded. Exiting.\"); exit()\n",
    "\n",
    "# \u2500\u2500\u2500 (C) NEW Feature Computation Functions \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def get_aa_composition(sequence_str):\n",
    "    composition = {aa: 0 for aa in canonical_set}\n",
    "    valid_len = 0\n",
    "    for aa in sequence_str:\n",
    "        if aa in canonical_set:\n",
    "            composition[aa] += 1\n",
    "            valid_len += 1\n",
    "    if valid_len == 0: return {aa: 0.0 for aa in canonical_set}, 0\n",
    "    for aa in composition: composition[aa] /= valid_len\n",
    "    return composition, valid_len\n",
    "\n",
    "def calculate_shannon_entropy(aa_composition):\n",
    "    entropy = 0.0\n",
    "    for aa_freq in aa_composition.values(): # Iterate over frequencies directly\n",
    "        if aa_freq > 0:\n",
    "            entropy -= aa_freq * math.log2(aa_freq)\n",
    "    return entropy\n",
    "\n",
    "def compute_new_seven_features(sequence_str):\n",
    "    if not sequence_str: return np.zeros(7)\n",
    "    composition, valid_seq_len = get_aa_composition(sequence_str)\n",
    "    if valid_seq_len == 0: return np.zeros(7)\n",
    "\n",
    "    hydro_norm_sum, flex_norm_sum, h_bond_potential_sum = 0, 0, 0\n",
    "    for aa in sequence_str:\n",
    "        if aa in aa_properties_base:\n",
    "            props = aa_properties_base[aa]\n",
    "            hydro_norm_sum += props['hydro_norm']\n",
    "            flex_norm_sum += props['flexibility'] / 0.544\n",
    "            h_bond_potential_sum += props['h_donors'] + props['h_acceptors']\n",
    "\n",
    "    net_charge_prop = (composition.get('R',0) + composition.get('K',0)) - \\\n",
    "                      (composition.get('D',0) + composition.get('E',0))\n",
    "    bulky_hydrophobics_list = ['W', 'C', 'F', 'Y', 'I', 'V', 'L']\n",
    "    \n",
    "    return np.array([\n",
    "        hydro_norm_sum / valid_seq_len,\n",
    "        flex_norm_sum / valid_seq_len,\n",
    "        h_bond_potential_sum / valid_seq_len,\n",
    "        abs(net_charge_prop),\n",
    "        calculate_shannon_entropy(composition),\n",
    "        composition.get('P', 0),\n",
    "        sum(composition.get(aa, 0) for aa in bulky_hydrophobics_list)\n",
    "    ])\n",
    "\n",
    "def compute_features_for_dataset(sequence_list, window_size_param=None):\n",
    "    \"\"\"\n",
    "    Computes the new 7 features for a list of sequence strings.\n",
    "    If window_size_param is None, computes global features.\n",
    "    If window_size_param is an int, computes features for each window and averages them.\n",
    "    \"\"\"\n",
    "    all_feature_vectors = []\n",
    "    for seq_str in sequence_list:\n",
    "        if not seq_str: \n",
    "            all_feature_vectors.append(np.zeros(7))\n",
    "            continue\n",
    "        \n",
    "        canonical_sequence = \"\".join([aa for aa in seq_str if aa in canonical_set])\n",
    "        if not canonical_sequence: \n",
    "            all_feature_vectors.append(np.zeros(7))\n",
    "            continue\n",
    "\n",
    "        if window_size_param is None or len(canonical_sequence) < window_size_param:\n",
    "            all_feature_vectors.append(compute_new_seven_features(canonical_sequence))\n",
    "        else:\n",
    "            window_derived_feature_sets = [] \n",
    "            for i in range(len(canonical_sequence) - window_size_param + 1):\n",
    "                window_segment_str = canonical_sequence[i : i + window_size_param]\n",
    "                window_features = compute_new_seven_features(window_segment_str)\n",
    "                window_derived_feature_sets.append(window_features)\n",
    "            if not window_derived_feature_sets:\n",
    "                all_feature_vectors.append(compute_new_seven_features(canonical_sequence)) # Fallback\n",
    "            else:\n",
    "                all_feature_vectors.append(np.mean(np.vstack(window_derived_feature_sets), axis=0))\n",
    "    return all_feature_vectors\n",
    "\n",
    "# --- Prepare data for Global New Features Classifier ---\n",
    "new_feature_names = [\n",
    "    \"hydro_norm_avg\", \"flex_norm_avg\", \"h_bond_potential_avg\",\n",
    "    \"abs_net_charge_prop\", \"shannon_entropy\", \"freq_proline\", \"freq_bulky_hydrophobics\"\n",
    "]\n",
    "print(f\"\\nComputing NEW GLOBAL features (WINDOW_SIZE_GLOBAL_FEATURES = {WINDOW_SIZE_GLOBAL_FEATURES})...\")\n",
    "raw_sequences_list = [item['sequence'] for item in all_sequences_data]\n",
    "labels_list = [item['label'] for item in all_sequences_data]\n",
    "\n",
    "global_features_calculated = compute_features_for_dataset(raw_sequences_list, window_size_param=WINDOW_SIZE_GLOBAL_FEATURES)\n",
    "\n",
    "df_global_features = pd.DataFrame(global_features_calculated, columns=new_feature_names)\n",
    "df_global_features[\"label\"] = labels_list\n",
    "print(\"NEW GLOBAL feature computation complete.\")\n",
    "\n",
    "if df_global_features.empty or df_global_features['label'].nunique() < 2:\n",
    "    print(\"Error: Not enough data for global features. Exiting.\"); exit()\n",
    "    \n",
    "X_global_train, X_global_test, y_global_train, y_global_test, train_indices_global, test_indices_global = train_test_split(\n",
    "    df_global_features.drop(columns=[\"label\"]),\n",
    "    df_global_features[\"label\"],\n",
    "    np.arange(len(raw_sequences_list)), \n",
    "    test_size=0.2, random_state=42, stratify=df_global_features[\"label\"] \n",
    ")\n",
    "\n",
    "df_train_global_features = X_global_train.copy()\n",
    "df_train_global_features[\"label\"] = y_global_train\n",
    "\n",
    "# Raw sequences for the test set (will be used by the sliding window classifier)\n",
    "test_raw_sequences_for_sliding_window = [raw_sequences_list[i] for i in test_indices_global]\n",
    "y_test_for_sliding_window = y_global_test # True labels for the test set\n",
    "\n",
    "print(f\"\\nGlobal Features Training set size: {len(df_train_global_features)}\")\n",
    "print(f\"Global Features Testing set size: {len(X_global_test)}\")\n",
    "\n",
    "# --- Calculate Midpoints from Global New Features Training Data ---\n",
    "if df_train_global_features[\"label\"].nunique() < 2:\n",
    "    print(\"\\nError: Global features training set lacks class diversity for midpoints.\"); exit()\n",
    "else:\n",
    "    train_means_global = df_train_global_features.groupby(\"label\").mean().rename(index={0:\"DisProt\", 1:\"PDB\"})\n",
    "    if \"PDB\" not in train_means_global.index or \"DisProt\" not in train_means_global.index:\n",
    "        print(\"\\nError: Could not find means for both PDB and DisProt in global training data.\"); exit()\n",
    "    else:\n",
    "        midpoints_global_new_features = {col: (train_means_global.loc[\"PDB\", col] + train_means_global.loc[\"DisProt\", col]) / 2\n",
    "                                         for col in X_global_train.columns}\n",
    "        print(\"\\nGlobal Feature Means (DisProt vs. PDB) from NEW GLOBAL FEATURES TRAINING DATA:\\n\")\n",
    "        print(train_means_global, \"\\n\")\n",
    "        print(\"Chosen Midpoint Thresholds (from NEW GLOBAL FEATURES TRAINING DATA):\\n\")\n",
    "        for feat, t in midpoints_global_new_features.items(): print(f\"  {feat:18s} = {t:.3f}\")\n",
    "        print()\n",
    "\n",
    "# --- Helper to count conditions met for the NEW 7 features ---\n",
    "def count_conditions_for_new_feature_vector(new_feature_vector_values, midpoints_dict, train_means_for_direction):\n",
    "    row = pd.Series(new_feature_vector_values, index=new_feature_names)\n",
    "    conditions_met_count = 0\n",
    "    \n",
    "    # hydro_norm_avg: PDB typically higher\n",
    "    if row[\"hydro_norm_avg\"] >= midpoints_dict.get(\"hydro_norm_avg\", 0.0): conditions_met_count +=1\n",
    "    # flex_norm_avg: PDB typically lower\n",
    "    if row[\"flex_norm_avg\"] <= midpoints_dict.get(\"flex_norm_avg\", float('inf')): conditions_met_count +=1\n",
    "    # h_bond_potential_avg: PDB typically lower\n",
    "    if row[\"h_bond_potential_avg\"] <= midpoints_dict.get(\"h_bond_potential_avg\", float('inf')): conditions_met_count +=1\n",
    "    # abs_net_charge_prop: PDB typically lower\n",
    "    if row[\"abs_net_charge_prop\"] <= midpoints_dict.get(\"abs_net_charge_prop\", float('inf')): conditions_met_count +=1\n",
    "    # shannon_entropy: PDB typically higher (inspect means to confirm this assumption)\n",
    "    if train_means_for_direction.loc[\"PDB\", \"shannon_entropy\"] > train_means_for_direction.loc[\"DisProt\", \"shannon_entropy\"]:\n",
    "        if row[\"shannon_entropy\"] >= midpoints_dict.get(\"shannon_entropy\", 0.0): conditions_met_count +=1\n",
    "    else: # PDB shannon_entropy is lower or equal\n",
    "        if row[\"shannon_entropy\"] <= midpoints_dict.get(\"shannon_entropy\", float('inf')): conditions_met_count +=1\n",
    "    # freq_proline: PDB typically lower\n",
    "    if row[\"freq_proline\"] <= midpoints_dict.get(\"freq_proline\", float('inf')): conditions_met_count +=1\n",
    "    # freq_bulky_hydrophobics: PDB typically higher\n",
    "    if row[\"freq_bulky_hydrophobics\"] >= midpoints_dict.get(\"freq_bulky_hydrophobics\", 0.0): conditions_met_count +=1\n",
    "    \n",
    "    return conditions_met_count\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# --- 1. New Global Features Threshold-Based Classifier Evaluation ---\n",
    "# ----------------------------------------------------------------------------------\n",
    "print(\"\\n\\n--- Evaluating New Global Features Threshold-Based Classifier ---\")\n",
    "df_test_global_features_eval = X_global_test.copy()\n",
    "df_test_global_features_eval[\"label\"] = y_global_test\n",
    "\n",
    "if df_test_global_features_eval.empty:\n",
    "    print(\"Global features test set is empty. Skipping evaluation.\")\n",
    "else:\n",
    "    df_test_global_features_eval[\"conditions_met\"] = df_test_global_features_eval.apply(\n",
    "        lambda r: count_conditions_for_new_feature_vector(r[new_feature_names].values, midpoints_global_new_features, train_means_global), axis=1\n",
    "    )\n",
    "    \n",
    "    dist_test_global = df_test_global_features_eval.groupby(\"label\")[\"conditions_met\"].value_counts().unstack(fill_value=0).rename(index={0:\"DisProt\", 1:\"PDB\"})\n",
    "    print(\"Distribution of \u2018conditions_met\u2019 (NEW GLOBAL features) by Label (ON TEST SET):\\n\")\n",
    "    print(dist_test_global, \"\\n\")\n",
    "\n",
    "    results_global = []\n",
    "    for k_thresh in range(1, 8):\n",
    "        preds_test_global = (df_test_global_features_eval[\"conditions_met\"] >= k_thresh).astype(int)\n",
    "        tp = ((preds_test_global == 1) & (y_global_test == 1)).sum()\n",
    "        fn = ((preds_test_global == 0) & (y_global_test == 1)).sum()\n",
    "        tn = ((preds_test_global == 0) & (y_global_test == 0)).sum()\n",
    "        fp = ((preds_test_global == 1) & (y_global_test == 0)).sum()\n",
    "        acc = (tp + tn) / len(y_global_test) if len(y_global_test) > 0 else 0\n",
    "        prec_pdb = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        rec_pdb = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_pdb = 2*(prec_pdb*rec_pdb)/(prec_pdb+rec_pdb) if (prec_pdb+rec_pdb)>0 else 0\n",
    "        results_global.append({\n",
    "            \"k\": k_thresh, \"TP\": tp, \"FN\": fn, \"TN\": tn, \"FP\": fp, \"Accuracy\": f\"{acc:.2%}\",\n",
    "            \"Precision (PDB)\": f\"{prec_pdb:.2%}\", \"Recall (PDB)\": f\"{rec_pdb:.2%}\", \"F1-score (PDB)\": f\"{f1_pdb:.2%}\"\n",
    "        })\n",
    "    df_results_global = pd.DataFrame(results_global)\n",
    "    print(\"Performance of New Global Features Classifier on TEST SET (varying k):\\n\")\n",
    "    print(df_results_global.to_string(index=False))\n",
    "\n",
    "    if not df_results_global.empty:\n",
    "        try:\n",
    "            df_results_global['F1_PDB_float'] = df_results_global['F1-score (PDB)'].str.rstrip('%').astype('float') / 100.0\n",
    "            best_k_row_global = df_results_global.loc[df_results_global['F1_PDB_float'].idxmax()]\n",
    "            best_k_global = int(best_k_row_global[\"k\"])\n",
    "            print(f\"\\n--- Detailed New Global Features Classification Report for best k = {best_k_global} (based on F1 PDB) ---\")\n",
    "            best_preds_global = (df_test_global_features_eval[\"conditions_met\"] >= best_k_global).astype(int)\n",
    "            print(classification_report(y_global_test, best_preds_global, target_names=[\"DisProt (0)\", \"PDB (1)\"], zero_division=0))\n",
    "            cm_global = confusion_matrix(y_global_test, best_preds_global)\n",
    "            print(\"Confusion Matrix for best k (New Global Features):\\n\", pd.DataFrame(cm_global, index=[\"Actual DisProt\",\"Actual PDB\"], columns=[\"Pred DisProt\",\"Pred PDB\"]))\n",
    "        except Exception as e: print(f\"Error in detailed report for global features: {e}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# --- 2. Sliding Window (Larger) Classifier with Failure Cancellation ---\n",
    "# ----------------------------------------------------------------------------------\n",
    "print(\"\\n\\n--- Testing Sliding Window (Larger) Classifier with Failure Cancellation ---\")\n",
    "\n",
    "def classify_protein_sliding_window_cancellation_new_features(\n",
    "    sequence_str, window_size, slide_step,\n",
    "    midpoints_for_eval, window_k_pass_thresh, \n",
    "    max_allowed_total_failures, train_means_for_direction_check): # Added train_means\n",
    "\n",
    "    if not sequence_str: return 1 \n",
    "    canonical_sequence = \"\".join([aa for aa in sequence_str if aa in canonical_set])\n",
    "    if not canonical_sequence or len(canonical_sequence) < window_size: return 1 \n",
    "\n",
    "    current_consecutive_failures_streak = 0\n",
    "    num_windows_processed = 0\n",
    "\n",
    "    for i in range(0, len(canonical_sequence) - window_size + 1, slide_step):\n",
    "        window_str = canonical_sequence[i : i + window_size]\n",
    "        num_windows_processed += 1\n",
    "        \n",
    "        seven_new_features_for_current_window = compute_new_seven_features(window_str)\n",
    "        num_conditions_this_window_met = count_conditions_for_new_feature_vector(\n",
    "            seven_new_features_for_current_window, \n",
    "            midpoints_for_eval,\n",
    "            train_means_for_direction_check # Pass train_means here\n",
    "        )\n",
    "        \n",
    "        window_passes = (num_conditions_this_window_met >= window_k_pass_thresh)\n",
    "        \n",
    "        if window_passes: current_consecutive_failures_streak = 0 \n",
    "        else: current_consecutive_failures_streak += 1\n",
    "            \n",
    "    if num_windows_processed == 0: return 0 \n",
    "    total_unforgiven_failures = current_consecutive_failures_streak\n",
    "    \n",
    "    return 1 if total_unforgiven_failures <= max_allowed_total_failures else 0\n",
    "\n",
    "print(f\"\\nApplying Sliding Window (Larger) Classifier (NEW features) with Failure Cancellation: window_size={SLIDING_WINDOW_SIZE}, slide_step={SLIDING_WINDOW_SLIDE_STEP}, window_k_pass_thresh={SLIDING_WINDOW_PASS_K}, max_total_unforgiven_failures={MAX_UNFORGIVEN_FAILED_WINDOWS_SLIDING}...\")\n",
    "predictions_sliding_window_test = []\n",
    "if not test_raw_sequences_for_sliding_window:\n",
    "    print(\"No raw sequences in test set for sliding window classifier.\")\n",
    "else:\n",
    "    for raw_seq in test_raw_sequences_for_sliding_window:\n",
    "        pred = classify_protein_sliding_window_cancellation_new_features(\n",
    "            raw_seq, SLIDING_WINDOW_SIZE, SLIDING_WINDOW_SLIDE_STEP,\n",
    "            midpoints_global_new_features, # Use midpoints from global new features training\n",
    "            SLIDING_WINDOW_PASS_K,\n",
    "            MAX_UNFORGIVEN_FAILED_WINDOWS_SLIDING,\n",
    "            train_means_global # Pass train_means for direction check\n",
    "        )\n",
    "        predictions_sliding_window_test.append(pred)\n",
    "    print(\"\\nSliding Window (Larger) with Failure Cancellation classification complete.\")\n",
    "\n",
    "    if predictions_sliding_window_test:\n",
    "        print(\"\\nPerformance of Sliding Window (Larger) Classifier with Failure Cancellation (ON TEST SET):\\n\")\n",
    "        print(classification_report(y_test_for_sliding_window, predictions_sliding_window_test, target_names=[\"DisProt (0)\", \"PDB (1)\"], zero_division=0))\n",
    "        cm_sliding = confusion_matrix(y_test_for_sliding_window, predictions_sliding_window_test)\n",
    "        print(\"Confusion Matrix:\\n\", pd.DataFrame(cm_sliding, index=[\"Actual DisProt\",\"Actual PDB\"], columns=[\"Pred DisProt\",\"Pred PDB\"]))\n",
    "        acc_sliding = (cm_sliding[0,0] + cm_sliding[1,1]) / np.sum(cm_sliding) if np.sum(cm_sliding) > 0 else 0\n",
    "        print(f\"Accuracy: {acc_sliding:.2%}\")\n",
    "    else:\n",
    "        print(\"No predictions made by Sliding Window (Larger) classifier.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Experiments\n",
    "\n",
    "This section implements rigorous validation controls to test if the concept model generalizes beyond the training distribution.\n",
    "\n",
    "## Task 1: MobiDB Independent Test Set\n",
    "\n",
    "MobiDB provides consensus disorder predictions and experimental annotations, serving as an independent validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobiDB Data Loader\nimport requests\nimport time\n\ndef load_mobidb_consensus(limit=1000, output_file=\"mobidb_test.fasta\"):\n    \"\"\"\n    Fetch consensus disorder predictions from MobiDB API.\n    Uses MobiDB-lite entries with experimentally validated disorder annotations.\n    \n    Args:\n        limit: Maximum number of sequences to fetch\n        output_file: Path to save FASTA file\n    \n    Returns:\n        List of (sequence, label) tuples where label is 0 (disordered) or 1 (ordered)\n    \"\"\"\n    print(f\"Fetching MobiDB consensus data (limit={limit})...\")\n    \n    sequences_with_labels = []\n    \n    # MobiDB API endpoint for searching entries\n    # We'll fetch both ordered and disordered proteins\n    base_url = \"https://mobidb.bio.unipd.it/api/search\"\n    \n    # Fetch disordered proteins (with high disorder content)\n    disordered_params = {\n        \"disorder_content_min\": 0.7,  # At least 70% disordered\n        \"limit\": limit // 2\n    }\n    \n    failed_fetches_disordered = 0\n    total_fetches_disordered = 0\n    \n    try:\n        print(\"Fetching disordered proteins from MobiDB...\")\n        resp = requests.get(base_url, params=disordered_params, timeout=60)\n        resp.raise_for_status()\n        disordered_data = resp.json()\n        \n        for entry in disordered_data.get('data', [])[:limit//2]:\n            acc = entry.get('acc')\n            if acc:\n                total_fetches_disordered += 1\n                # Fetch sequence\n                seq_url = f\"https://mobidb.bio.unipd.it/api/download?acc={acc}&format=fasta\"\n                try:\n                    seq_resp = requests.get(seq_url, timeout=30)\n                    if seq_resp.status_code == 200:\n                        fasta_text = seq_resp.text.strip()\n                        lines = fasta_text.split('\\n')\n                        if len(lines) > 1:\n                            sequence = ''.join(lines[1:])\n                            sequences_with_labels.append((sequence, 0))  # 0 for disordered\n                        else:\n                            failed_fetches_disordered += 1\n                    else:\n                        failed_fetches_disordered += 1\n                except Exception as seq_err:\n                    failed_fetches_disordered += 1\n                    print(f\"  Warning: Failed to fetch sequence for {acc}: {seq_err}\")\n                time.sleep(0.2)  # Rate limiting\n        \n        if total_fetches_disordered > 0:\n            failure_rate = failed_fetches_disordered / total_fetches_disordered\n            if failure_rate > 0.3:\n                print(f\"  \u26a0 Warning: High failure rate for disordered proteins ({failed_fetches_disordered}/{total_fetches_disordered} = {failure_rate:.1%})\")\n                print(f\"  Dataset may be imbalanced.\")\n    except Exception as e:\n        print(f\"Warning: Error fetching disordered proteins: {e}\")\n    \n    # Fetch ordered proteins (with low disorder content)\n    ordered_params = {\n        \"disorder_content_max\": 0.1,  # At most 10% disordered\n        \"limit\": limit // 2\n    }\n    \n    failed_fetches_ordered = 0\n    total_fetches_ordered = 0\n    \n    try:\n        print(\"Fetching ordered proteins from MobiDB...\")\n        resp = requests.get(base_url, params=ordered_params, timeout=60)\n        resp.raise_for_status()\n        ordered_data = resp.json()\n        \n        for entry in ordered_data.get('data', [])[:limit//2]:\n            acc = entry.get('acc')\n            if acc:\n                total_fetches_ordered += 1\n                # Fetch sequence\n                seq_url = f\"https://mobidb.bio.unipd.it/api/download?acc={acc}&format=fasta\"\n                try:\n                    seq_resp = requests.get(seq_url, timeout=30)\n                    if seq_resp.status_code == 200:\n                        fasta_text = seq_resp.text.strip()\n                        lines = fasta_text.split('\\n')\n                        if len(lines) > 1:\n                            sequence = ''.join(lines[1:])\n                            sequences_with_labels.append((sequence, 1))  # 1 for ordered\n                        else:\n                            failed_fetches_ordered += 1\n                    else:\n                        failed_fetches_ordered += 1\n                except Exception as seq_err:\n                    failed_fetches_ordered += 1\n                    print(f\"  Warning: Failed to fetch sequence for {acc}: {seq_err}\")\n                time.sleep(0.2)  # Rate limiting\n        \n        if total_fetches_ordered > 0:\n            failure_rate = failed_fetches_ordered / total_fetches_ordered\n            if failure_rate > 0.3:\n                print(f\"  \u26a0 Warning: High failure rate for ordered proteins ({failed_fetches_ordered}/{total_fetches_ordered} = {failure_rate:.1%})\")\n                print(f\"  Dataset may be imbalanced.\")\n    except Exception as e:\n        print(f\"Warning: Error fetching ordered proteins: {e}\")\n    \n    # Save to FASTA file\n    if sequences_with_labels:\n        with open(output_file, 'w') as f:\n            for i, (seq, label) in enumerate(sequences_with_labels):\n                label_str = \"ordered\" if label == 1 else \"disordered\"\n                f.write(f\">mobidb_{label_str}_{i+1}\\n\")\n                f.write(f\"{seq}\\n\")\n        print(f\"\u2714 Saved {len(sequences_with_labels)} MobiDB sequences to '{output_file}'\")\n        print(f\"  - Ordered: {sum(1 for s, l in sequences_with_labels if l == 1)}\")\n        print(f\"  - Disordered: {sum(1 for s, l in sequences_with_labels if l == 0)}\")\n    else:\n        print(\"Warning: No sequences fetched from MobiDB\")\n    \n    return sequences_with_labels\n\n# Test the MobiDB loader (commented out by default to avoid API calls)\n# mobidb_data = load_mobidb_consensus(limit=100)\nprint(\"MobiDB loader function defined. Uncomment the line above to fetch data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Homology-Aware Cross-Validation\n",
    "\n",
    "Implements sequence clustering to prevent homology leakage between train and test sets.\n",
    "Uses MMseqs2 for fast sequence clustering at 30% identity threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homology-Aware Cross-Validation\nimport subprocess\nimport tempfile\nimport os\nimport random\nfrom collections import defaultdict\n\ndef install_mmseqs2():\n    \"\"\"\n    Install MMseqs2 if not already available.\n    Checks for conda availability before attempting installation.\n    \"\"\"\n    try:\n        subprocess.run(['mmseqs', 'version'], capture_output=True, check=True)\n        print(\"MMseqs2 is already installed.\")\n        return True\n    except (FileNotFoundError, subprocess.CalledProcessError):\n        print(\"MMseqs2 not found. Checking for conda...\")\n        \n        # Check if conda is available\n        try:\n            subprocess.run(['conda', '--version'], capture_output=True, check=True)\n            print(\"Conda found. Installing MMseqs2 via conda...\")\n        except (FileNotFoundError, subprocess.CalledProcessError):\n            print(\"Conda not available. Cannot install MMseqs2 automatically.\")\n            print(\"Falling back to simple random split.\")\n            return False\n        \n        # Try to install MMseqs2\n        try:\n            result = subprocess.run(\n                ['conda', 'install', '-y', '-c', 'bioconda', 'mmseqs2'], \n                capture_output=True, \n                check=True,\n                timeout=300  # 5 minute timeout\n            )\n            print(\"\u2714 MMseqs2 installed successfully.\")\n            return True\n        except subprocess.TimeoutExpired:\n            print(\"Warning: MMseqs2 installation timed out.\")\n            print(\"Falling back to simple random split.\")\n            return False\n        except Exception as e:\n            print(f\"Warning: Could not install MMseqs2: {e}\")\n            print(\"Falling back to simple random split.\")\n            return False\n\ndef cluster_sequences_mmseqs2(sequences, identity_threshold=0.3):\n    \"\"\"\n    Cluster sequences using MMseqs2.\n    \n    Args:\n        sequences: List of (header, sequence) tuples\n        identity_threshold: Sequence identity threshold (0.0-1.0)\n    \n    Returns:\n        Dictionary mapping sequence index to cluster ID\n    \"\"\"\n    if not sequences:\n        return {}\n    \n    # Create temporary directory for MMseqs2\n    with tempfile.TemporaryDirectory() as tmpdir:\n        # Write sequences to FASTA\n        fasta_path = os.path.join(tmpdir, 'sequences.fasta')\n        with open(fasta_path, 'w') as f:\n            for i, (header, seq) in enumerate(sequences):\n                f.write(f\">seq_{i}\\n{seq}\\n\")\n        \n        # Run MMseqs2 clustering\n        db_path = os.path.join(tmpdir, 'seqDB')\n        cluster_path = os.path.join(tmpdir, 'clusterDB')\n        tmp_path = os.path.join(tmpdir, 'tmp')\n        os.makedirs(tmp_path, exist_ok=True)\n        \n        try:\n            # Create sequence database\n            subprocess.run(['mmseqs', 'createdb', fasta_path, db_path],\n                         check=True, capture_output=True)\n            \n            # Cluster sequences\n            subprocess.run(['mmseqs', 'cluster', db_path, cluster_path, tmp_path,\n                          '--min-seq-id', str(identity_threshold), '-c', '0.8'],\n                         check=True, capture_output=True)\n            \n            # Create TSV output\n            tsv_path = os.path.join(tmpdir, 'clusters.tsv')\n            subprocess.run(['mmseqs', 'createtsv', db_path, db_path, cluster_path, tsv_path],\n                         check=True, capture_output=True)\n            \n            # Parse cluster assignments\n            seq_to_cluster = {}\n            with open(tsv_path, 'r') as f:\n                for line in f:\n                    parts = line.strip().split('\\t')\n                    if len(parts) >= 2:\n                        representative = parts[0].replace('seq_', '')\n                        member = parts[1].replace('seq_', '')\n                        seq_to_cluster[int(member)] = int(representative)\n            \n            return seq_to_cluster\n            \n        except subprocess.CalledProcessError as e:\n            print(f\"Warning: MMseqs2 clustering failed: {e}\")\n            # Fallback: assign each sequence to its own cluster\n            return {i: i for i in range(len(sequences))}\n\ndef homology_aware_split(sequences, labels, test_size=0.2, identity_threshold=0.3, random_state=42):\n    \"\"\"\n    Split sequences into train/test sets ensuring sequence clusters stay together.\n    \n    Args:\n        sequences: List of protein sequences (strings)\n        labels: List of labels (0 or 1)\n        test_size: Fraction of data for test set (0.0-1.0)\n        identity_threshold: Sequence identity threshold for clustering\n        random_state: Random seed for reproducibility\n    \n    Returns:\n        Tuple of (train_indices, test_indices)\n    \"\"\"\n    random.seed(random_state)\n    np.random.seed(random_state)\n    \n    print(f\"\\nPerforming homology-aware split (identity threshold={identity_threshold})...\")\n    \n    # Check if MMseqs2 is available\n    mmseqs_available = install_mmseqs2()\n    \n    if mmseqs_available:\n        # Cluster sequences\n        seq_tuples = [(f\"seq_{i}\", seq) for i, seq in enumerate(sequences)]\n        seq_to_cluster = cluster_sequences_mmseqs2(seq_tuples, identity_threshold)\n    else:\n        # Fallback: each sequence is its own cluster (equivalent to random split)\n        print(\"Using fallback: simple random split without clustering.\")\n        seq_to_cluster = {i: i for i in range(len(sequences))}\n    \n    # Group sequences by cluster\n    cluster_to_seqs = defaultdict(list)\n    for seq_idx, cluster_id in seq_to_cluster.items():\n        cluster_to_seqs[cluster_id].append(seq_idx)\n    \n    print(f\"Found {len(cluster_to_seqs)} clusters from {len(sequences)} sequences\")\n    \n    # Separate clusters by label for stratified splitting\n    clusters_label_0 = []\n    clusters_label_1 = []\n    \n    for cluster_id, seq_indices in cluster_to_seqs.items():\n        # Determine majority label for this cluster\n        cluster_labels = [labels[i] for i in seq_indices]\n        majority_label = 1 if sum(cluster_labels) > len(cluster_labels) / 2 else 0\n        \n        if majority_label == 0:\n            clusters_label_0.append(seq_indices)\n        else:\n            clusters_label_1.append(seq_indices)\n    \n    # Shuffle clusters\n    random.shuffle(clusters_label_0)\n    random.shuffle(clusters_label_1)\n    \n    # Split clusters into train/test\n    n_test_0 = int(len(clusters_label_0) * test_size)\n    n_test_1 = int(len(clusters_label_1) * test_size)\n    \n    test_clusters_0 = clusters_label_0[:n_test_0]\n    train_clusters_0 = clusters_label_0[n_test_0:]\n    \n    test_clusters_1 = clusters_label_1[:n_test_1]\n    train_clusters_1 = clusters_label_1[n_test_1:]\n    \n    # Flatten clusters to get indices\n    train_indices = []\n    for cluster in train_clusters_0 + train_clusters_1:\n        train_indices.extend(cluster)\n    \n    test_indices = []\n    for cluster in test_clusters_0 + test_clusters_1:\n        test_indices.extend(cluster)\n    \n    print(f\"Split complete: {len(train_indices)} train, {len(test_indices)} test\")\n    print(f\"Train labels: {sum(labels[i] for i in train_indices)} folded, \"\n          f\"{len(train_indices) - sum(labels[i] for i in train_indices)} disordered\")\n    print(f\"Test labels: {sum(labels[i] for i in test_indices)} folded, \"\n          f\"{len(test_indices) - sum(labels[i] for i in test_indices)} disordered\")\n    \n    return train_indices, test_indices\n\nprint(\"Homology-aware split functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Label-Shuffle Control Experiment\n",
    "\n",
    "Tests for data leakage and overfitting by randomly permuting labels.\n",
    "Expected result: Performance should drop to ~50% (random chance) if the model is not overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label-Shuffle Control Experiment\nimport random\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef run_label_shuffle_control(sequences_data, n_iterations=5, random_state=42):\n    \"\"\"\n    Run classification experiment with shuffled labels as a control.\n    \n    Args:\n        sequences_data: List of dicts with 'sequence' and 'label' keys\n        n_iterations: Number of shuffle iterations to average\n        random_state: Random seed for reproducibility\n    \n    Returns:\n        Dictionary with average performance metrics\n    \"\"\"\n    print(f\"\\nRunning label-shuffle control experiment ({n_iterations} iterations)...\")\n    print(\"Expected result: ~50% accuracy if model is not overfitting.\\n\")\n    \n    all_accuracies = []\n    all_precisions = []\n    all_recalls = []\n    all_f1s = []\n    \n    for iteration in range(n_iterations):\n        # Set random seed for this iteration\n        iter_seed = random_state + iteration\n        random.seed(iter_seed)\n        np.random.seed(iter_seed)\n        \n        # Create shuffled labels\n        original_labels = [item['label'] for item in sequences_data]\n        shuffled_labels = original_labels.copy()\n        random.shuffle(shuffled_labels)\n        \n        # Create shuffled dataset\n        shuffled_data = []\n        for i, item in enumerate(sequences_data):\n            shuffled_data.append({\n                'sequence': item['sequence'],\n                'label': shuffled_labels[i],\n                'header': item.get('header', f'seq_{i}')\n            })\n        \n        # Extract sequences and labels\n        sequences_list = [item['sequence'] for item in shuffled_data]\n        labels_list = [item['label'] for item in shuffled_data]\n        \n        # Compute features\n        features = compute_features_for_dataset(sequences_list, window_size_param=None)\n        df_features = pd.DataFrame(features, columns=new_feature_names)\n        df_features['label'] = labels_list\n        \n        # Train/test split\n        X_train, X_test, y_train, y_test = train_test_split(\n            df_features.drop(columns=['label']),\n            df_features['label'],\n            test_size=0.2,\n            random_state=iter_seed,\n            stratify=df_features['label']\n        )\n        \n        # Compute midpoints from training data\n        train_label_0 = X_train[y_train == 0]\n        train_label_1 = X_train[y_train == 1]\n        \n        midpoints = []\n        train_means_label_0 = train_label_0.mean()\n        train_means_label_1 = train_label_1.mean()\n        \n        for feat in new_feature_names:\n            midpoint = (train_means_label_0[feat] + train_means_label_1[feat]) / 2\n            midpoints.append(midpoint)\n        \n        # Classify using feature count method (using helper from validation runner)\n        # Note: This assumes CLASSIFICATION_THRESHOLD and classify_using_feature_count \n        # are defined in the validation runner cell above\n        predictions = classify_using_feature_count(\n            X_test, new_feature_names, midpoints,\n            train_means_label_0, train_means_label_1,\n            threshold=5  # Use best threshold (K=5)\n        )\n        \n        # Calculate metrics\n        accuracy = accuracy_score(y_test, predictions)\n        precision, recall, f1, _ = precision_recall_fscore_support(\n            y_test, predictions, average='binary', zero_division=0\n        )\n        \n        all_accuracies.append(accuracy)\n        all_precisions.append(precision)\n        all_recalls.append(recall)\n        all_f1s.append(f1)\n        \n        print(f\"  Iteration {iteration+1}: Accuracy = {accuracy:.4f}, F1 = {f1:.4f}\")\n    \n    # Calculate average metrics\n    results = {\n        'accuracy': np.mean(all_accuracies),\n        'accuracy_std': np.std(all_accuracies),\n        'precision': np.mean(all_precisions),\n        'precision_std': np.std(all_precisions),\n        'recall': np.mean(all_recalls),\n        'recall_std': np.std(all_recalls),\n        'f1': np.mean(all_f1s),\n        'f1_std': np.std(all_f1s)\n    }\n    \n    print(f\"\\nLabel-Shuffle Control Results (averaged over {n_iterations} iterations):\")\n    print(f\"  Accuracy: {results['accuracy']:.4f} \u00b1 {results['accuracy_std']:.4f}\")\n    print(f\"  Precision: {results['precision']:.4f} \u00b1 {results['precision_std']:.4f}\")\n    print(f\"  Recall: {results['recall']:.4f} \u00b1 {results['recall_std']:.4f}\")\n    print(f\"  F1-score: {results['f1']:.4f} \u00b1 {results['f1_std']:.4f}\")\n    \n    if results['accuracy'] < 0.6:\n        print(\"\\n\u2713 PASS: Accuracy dropped to near-random levels, indicating no data leakage.\")\n    else:\n        print(\"\\n\u26a0 WARNING: Accuracy remains high with shuffled labels, suggesting possible data leakage or overfitting.\")\n    \n    return results\n\n# Run the control experiment (commented out by default)\n# shuffle_results = run_label_shuffle_control(all_sequences_data, n_iterations=5)\nprint(\"Label-shuffle control function defined. Uncomment the line above to run the experiment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Statistical Significance Testing\n",
    "\n",
    "Implements bootstrap confidence intervals and McNemar's test to assess statistical significance of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Significance Testing\n",
    "from scipy.stats import chi2\n",
    "\n",
    "def bootstrap_confidence_interval(y_true, y_pred, n_bootstrap=1000, confidence_level=0.95, random_state=42):\n",
    "    \"\"\"\n",
    "    Compute bootstrap confidence interval for accuracy.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "        n_bootstrap: Number of bootstrap samples\n",
    "        confidence_level: Confidence level (e.g., 0.95 for 95% CI)\n",
    "        random_state: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with mean accuracy and confidence interval\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    n_samples = len(y_true)\n",
    "    \n",
    "    bootstrap_accuracies = []\n",
    "    \n",
    "    for i in range(n_bootstrap):\n",
    "        # Sample with replacement\n",
    "        indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "        bootstrap_accuracy = accuracy_score(y_true[indices], y_pred[indices])\n",
    "        bootstrap_accuracies.append(bootstrap_accuracy)\n",
    "    \n",
    "    bootstrap_accuracies = np.array(bootstrap_accuracies)\n",
    "    \n",
    "    # Calculate confidence interval\n",
    "    alpha = 1 - confidence_level\n",
    "    lower_percentile = (alpha / 2) * 100\n",
    "    upper_percentile = (1 - alpha / 2) * 100\n",
    "    \n",
    "    ci_lower = np.percentile(bootstrap_accuracies, lower_percentile)\n",
    "    ci_upper = np.percentile(bootstrap_accuracies, upper_percentile)\n",
    "    \n",
    "    return {\n",
    "        'mean': np.mean(bootstrap_accuracies),\n",
    "        'std': np.std(bootstrap_accuracies),\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'confidence_level': confidence_level\n",
    "    }\n",
    "\n",
    "def mcnemar_test(y_true, predictions1, predictions2):\n",
    "    \"\"\"\n",
    "    Perform McNemar's test to compare two classifiers.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        predictions1: Predictions from first classifier\n",
    "        predictions2: Predictions from second classifier\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with test statistic and p-value\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    predictions1 = np.array(predictions1)\n",
    "    predictions2 = np.array(predictions2)\n",
    "    \n",
    "    # Create contingency table\n",
    "    # n01: classifier 1 wrong, classifier 2 correct\n",
    "    # n10: classifier 1 correct, classifier 2 wrong\n",
    "    correct1 = (predictions1 == y_true)\n",
    "    correct2 = (predictions2 == y_true)\n",
    "    \n",
    "    n01 = np.sum(~correct1 & correct2)\n",
    "    n10 = np.sum(correct1 & ~correct2)\n",
    "    \n",
    "    # McNemar's test statistic with continuity correction\n",
    "    if n01 + n10 == 0:\n",
    "        return {\n",
    "            'statistic': 0.0,\n",
    "            'p_value': 1.0,\n",
    "            'n01': n01,\n",
    "            'n10': n10,\n",
    "            'interpretation': 'Classifiers are identical'\n",
    "        }\n",
    "    \n",
    "    statistic = (abs(n01 - n10) - 1) ** 2 / (n01 + n10)\n",
    "    p_value = 1 - chi2.cdf(statistic, df=1)\n",
    "    \n",
    "    # Interpretation\n",
    "    if p_value < 0.05:\n",
    "        interpretation = 'Classifiers are significantly different (p < 0.05)'\n",
    "    else:\n",
    "        interpretation = 'No significant difference between classifiers (p >= 0.05)'\n",
    "    \n",
    "    return {\n",
    "        'statistic': statistic,\n",
    "        'p_value': p_value,\n",
    "        'n01': n01,\n",
    "        'n10': n10,\n",
    "        'interpretation': interpretation\n",
    "    }\n",
    "\n",
    "def compute_statistical_significance(predictions1, predictions2, labels, \n",
    "                                     method1_name=\"Method 1\", method2_name=\"Method 2\",\n",
    "                                     n_bootstrap=1000):\n",
    "    \"\"\"\n",
    "    Compute statistical significance between two sets of predictions.\n",
    "    \n",
    "    Args:\n",
    "        predictions1: Predictions from first method\n",
    "        predictions2: Predictions from second method\n",
    "        labels: True labels\n",
    "        method1_name: Name of first method\n",
    "        method2_name: Name of second method\n",
    "        n_bootstrap: Number of bootstrap samples\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with all statistical test results\n",
    "    \"\"\"\n",
    "    print(f\"\\nComputing statistical significance: {method1_name} vs {method2_name}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Bootstrap CI for method 1\n",
    "    print(f\"\\nBootstrap confidence intervals ({n_bootstrap} samples):\")\n",
    "    ci1 = bootstrap_confidence_interval(labels, predictions1, n_bootstrap=n_bootstrap)\n",
    "    print(f\"  {method1_name}:\")\n",
    "    print(f\"    Accuracy: {ci1['mean']:.4f} \u00b1 {ci1['std']:.4f}\")\n",
    "    print(f\"    95% CI: [{ci1['ci_lower']:.4f}, {ci1['ci_upper']:.4f}]\")\n",
    "    \n",
    "    # Bootstrap CI for method 2\n",
    "    ci2 = bootstrap_confidence_interval(labels, predictions2, n_bootstrap=n_bootstrap)\n",
    "    print(f\"  {method2_name}:\")\n",
    "    print(f\"    Accuracy: {ci2['mean']:.4f} \u00b1 {ci2['std']:.4f}\")\n",
    "    print(f\"    95% CI: [{ci2['ci_lower']:.4f}, {ci2['ci_upper']:.4f}]\")\n",
    "    \n",
    "    # McNemar's test\n",
    "    print(f\"\\nMcNemar's test:\")\n",
    "    mcnemar_result = mcnemar_test(labels, predictions1, predictions2)\n",
    "    print(f\"  Test statistic: {mcnemar_result['statistic']:.4f}\")\n",
    "    print(f\"  P-value: {mcnemar_result['p_value']:.4f}\")\n",
    "    print(f\"  {mcnemar_result['interpretation']}\")\n",
    "    print(f\"  Contingency: {method1_name} wrong & {method2_name} correct = {mcnemar_result['n01']}\")\n",
    "    print(f\"               {method1_name} correct & {method2_name} wrong = {mcnemar_result['n10']}\")\n",
    "    \n",
    "    return {\n",
    "        'method1_ci': ci1,\n",
    "        'method2_ci': ci2,\n",
    "        'mcnemar': mcnemar_result\n",
    "    }\n",
    "\n",
    "print(\"Statistical significance functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Performance Comparison and Comprehensive Validation\n",
    "\n",
    "Run all validation experiments and create a comprehensive comparison table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Validation Runner\n\n# Classification threshold constant\nCLASSIFICATION_THRESHOLD = 5  # Number of features that must meet the \"folded\" condition\n\ndef classify_using_feature_count(X_test, new_feature_names, midpoints, train_means_label_0, train_means_label_1, threshold=CLASSIFICATION_THRESHOLD):\n    \"\"\"\n    Classify sequences using feature count method.\n    \n    Args:\n        X_test: Test features DataFrame\n        new_feature_names: List of feature names\n        midpoints: List of midpoint values for each feature\n        train_means_label_0: Mean values for label 0 (disordered)\n        train_means_label_1: Mean values for label 1 (ordered)\n        threshold: Number of features that must meet conditions for \"folded\" classification\n    \n    Returns:\n        List of predictions (0 or 1)\n    \"\"\"\n    predictions = []\n    for _, test_row in X_test.iterrows():\n        satisfied_count = 0\n        for feat_idx, feat_name in enumerate(new_feature_names):\n            feat_val = test_row[feat_name]\n            midpoint_val = midpoints[feat_idx]\n            \n            # Determine direction based on which class has higher mean\n            if train_means_label_1[feat_name] >= train_means_label_0[feat_name]:\n                if feat_val >= midpoint_val:\n                    satisfied_count += 1\n            else:\n                if feat_val <= midpoint_val:\n                    satisfied_count += 1\n        \n        predictions.append(1 if satisfied_count >= threshold else 0)\n    \n    return predictions\n\ndef run_classification_experiment(X_train, X_test, y_train, y_test, experiment_name=\"Experiment\"):\n    \"\"\"\n    Run the classification experiment with given train/test split.\n    \n    Args:\n        X_train: Training features\n        X_test: Test features\n        y_train: Training labels\n        y_test: Test labels\n        experiment_name: Name for this experiment\n    \n    Returns:\n        Dictionary with predictions and metrics\n    \"\"\"\n    # Compute midpoints from training data\n    train_label_0 = X_train[y_train == 0]\n    train_label_1 = X_train[y_train == 1]\n    \n    if len(train_label_0) == 0 or len(train_label_1) == 0:\n        print(f\"Warning: {experiment_name} - Insufficient data in one class\")\n        return None\n    \n    midpoints = []\n    train_means_label_0 = train_label_0.mean()\n    train_means_label_1 = train_label_1.mean()\n    \n    for feat in new_feature_names:\n        midpoint = (train_means_label_0[feat] + train_means_label_1[feat]) / 2\n        midpoints.append(midpoint)\n    \n    # Use the helper function for classification\n    predictions = classify_using_feature_count(\n        X_test, new_feature_names, midpoints, \n        train_means_label_0, train_means_label_1\n    )\n    \n    # Calculate metrics\n    accuracy = accuracy_score(y_test, predictions)\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        y_test, predictions, average='binary', zero_division=0\n    )\n    \n    return {\n        'predictions': predictions,\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    }\n\ndef run_all_validation_experiments():\n    \"\"\"\n    Run all validation experiments and create comparison table.\n    \n    Returns:\n        DataFrame with comparison results\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"COMPREHENSIVE VALIDATION EXPERIMENTS\")\n    print(\"=\"*80)\n    \n    results_list = []\n    \n    # Prepare data\n    sequences_list = [item['sequence'] for item in all_sequences_data]\n    labels_list = [item['label'] for item in all_sequences_data]\n    \n    # Compute features for all sequences\n    print(\"\\nComputing features for all sequences...\")\n    all_features = compute_features_for_dataset(sequences_list, window_size_param=None)\n    df_all = pd.DataFrame(all_features, columns=new_feature_names)\n    df_all['label'] = labels_list\n    \n    # Experiment 1: Original random split\n    print(\"\\n[1/4] Running original random split experiment...\")\n    X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n        df_all.drop(columns=['label']),\n        df_all['label'],\n        test_size=0.2,\n        random_state=42,\n        stratify=df_all['label']\n    )\n    \n    result_orig = run_classification_experiment(\n        X_train_orig, X_test_orig, y_train_orig, y_test_orig,\n        \"Original Split\"\n    )\n    \n    if result_orig:\n        results_list.append({\n            'Dataset': 'Original PDB/DisProt Split',\n            'Accuracy': f\"{result_orig['accuracy']:.4f}\",\n            'Precision': f\"{result_orig['precision']:.4f}\",\n            'Recall': f\"{result_orig['recall']:.4f}\",\n            'F1-Score': f\"{result_orig['f1']:.4f}\"\n        })\n        print(f\"  Accuracy: {result_orig['accuracy']:.4f}, F1: {result_orig['f1']:.4f}\")\n    \n    # Experiment 2: Homology-aware split\n    print(\"\\n[2/4] Running homology-aware split experiment...\")\n    try:\n        train_indices, test_indices = homology_aware_split(\n            sequences_list, labels_list,\n            test_size=0.2,\n            identity_threshold=0.3,\n            random_state=42\n        )\n        \n        X_train_homology = df_all.drop(columns=['label']).iloc[train_indices]\n        X_test_homology = df_all.drop(columns=['label']).iloc[test_indices]\n        y_train_homology = df_all['label'].iloc[train_indices]\n        y_test_homology = df_all['label'].iloc[test_indices]\n        \n        result_homology = run_classification_experiment(\n            X_train_homology, X_test_homology, y_train_homology, y_test_homology,\n            \"Homology-Aware Split\"\n        )\n        \n        if result_homology:\n            results_list.append({\n                'Dataset': 'Homology-Aware Split (30% identity)',\n                'Accuracy': f\"{result_homology['accuracy']:.4f}\",\n                'Precision': f\"{result_homology['precision']:.4f}\",\n                'Recall': f\"{result_homology['recall']:.4f}\",\n                'F1-Score': f\"{result_homology['f1']:.4f}\"\n            })\n            print(f\"  Accuracy: {result_homology['accuracy']:.4f}, F1: {result_homology['f1']:.4f}\")\n            \n            # Statistical comparison using bootstrap confidence intervals\n            # Since test sets differ, we can't use paired tests like McNemar's.\n            # Instead, we compare accuracy distributions using bootstrap CIs.\n            print(\"\\n  Computing bootstrap confidence intervals for comparison...\")\n            \n            ci_orig = bootstrap_confidence_interval(\n                y_test_orig.tolist() if hasattr(y_test_orig, 'tolist') else y_test_orig,\n                result_orig['predictions'],\n                n_bootstrap=1000\n            )\n            ci_homology = bootstrap_confidence_interval(\n                y_test_homology.tolist() if hasattr(y_test_homology, 'tolist') else y_test_homology,\n                result_homology['predictions'],\n                n_bootstrap=1000\n            )\n            \n            print(f\"  Original Split - Accuracy: {ci_orig['mean']:.4f} [{ci_orig['ci_lower']:.4f}, {ci_orig['ci_upper']:.4f}]\")\n            print(f\"  Homology-Aware - Accuracy: {ci_homology['mean']:.4f} [{ci_homology['ci_lower']:.4f}, {ci_homology['ci_upper']:.4f}]\")\n            \n            # Check if confidence intervals overlap\n            if ci_orig['ci_lower'] > ci_homology['ci_upper']:\n                print(f\"  \u2192 Original split significantly better (non-overlapping CIs)\")\n            elif ci_homology['ci_lower'] > ci_orig['ci_upper']:\n                print(f\"  \u2192 Homology-aware split significantly better (non-overlapping CIs)\")\n            else:\n                print(f\"  \u2192 No significant difference (overlapping CIs)\")\n    except Exception as e:\n        print(f\"  Error in homology-aware split: {e}\")\n        results_list.append({\n            'Dataset': 'Homology-Aware Split (30% identity)',\n            'Accuracy': 'Error',\n            'Precision': 'Error',\n            'Recall': 'Error',\n            'F1-Score': 'Error'\n        })\n    \n    # Experiment 3: Label-shuffle control\n    print(\"\\n[3/4] Running label-shuffle control experiment...\")\n    try:\n        shuffle_results = run_label_shuffle_control(all_sequences_data, n_iterations=5)\n        results_list.append({\n            'Dataset': 'Label-Shuffle Control',\n            'Accuracy': f\"{shuffle_results['accuracy']:.4f} \u00b1 {shuffle_results['accuracy_std']:.4f}\",\n            'Precision': f\"{shuffle_results['precision']:.4f} \u00b1 {shuffle_results['precision_std']:.4f}\",\n            'Recall': f\"{shuffle_results['recall']:.4f} \u00b1 {shuffle_results['recall_std']:.4f}\",\n            'F1-Score': f\"{shuffle_results['f1']:.4f} \u00b1 {shuffle_results['f1_std']:.4f}\"\n        })\n    except Exception as e:\n        print(f\"  Error in label-shuffle: {e}\")\n        results_list.append({\n            'Dataset': 'Label-Shuffle Control',\n            'Accuracy': 'Error',\n            'Precision': 'Error',\n            'Recall': 'Error',\n            'F1-Score': 'Error'\n        })\n    \n    # Experiment 4: MobiDB independent test (placeholder - requires API call)\n    print(\"\\n[4/4] MobiDB independent test (skipped - requires API call)\")\n    print(\"  To run: Uncomment load_mobidb_consensus() call above and add test code here\")\n    results_list.append({\n        'Dataset': 'MobiDB Independent Test',\n        'Accuracy': 'Not run',\n        'Precision': 'Not run',\n        'Recall': 'Not run',\n        'F1-Score': 'Not run'\n    })\n    \n    # Create comparison DataFrame\n    df_comparison = pd.DataFrame(results_list)\n    \n    return df_comparison\n\n# Run all experiments (commented out by default)\n# comparison_results = run_all_validation_experiments()\n# print(\"\\n\" + \"=\"*80)\n# print(\"PERFORMANCE COMPARISON TABLE\")\n# print(\"=\"*80)\n# print(comparison_results.to_markdown(index=False))\nprint(\"Comprehensive validation functions defined. Uncomment the lines above to run all experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Interpretation\n",
    "\n",
    "### Expected Outcomes\n",
    "\n",
    "If the concept model truly captures biophysical folding principles:\n",
    "\n",
    "1. **Homology-aware CV should maintain >75% accuracy**\n",
    "   - This demonstrates the model generalizes beyond memorizing similar sequences\n",
    "   - Any significant drop suggests sequence homology leakage in original split\n",
    "\n",
    "2. **MobiDB independent test should show >70% accuracy**\n",
    "   - MobiDB provides consensus predictions from multiple methods\n",
    "   - Good performance here validates cross-dataset generalization\n",
    "   - Lower performance may indicate PDB/DisProt-specific biases\n",
    "\n",
    "3. **Label-shuffle control should drop to ~50% accuracy**\n",
    "   - Random labels should yield random-chance performance\n",
    "   - High accuracy with shuffled labels indicates data leakage or feature artifacts\n",
    "   - This is a critical sanity check for any ML model\n",
    "\n",
    "4. **Statistical tests should show significant differences**\n",
    "   - Bootstrap confidence intervals quantify uncertainty\n",
    "   - McNemar's test compares different split strategies\n",
    "   - p < 0.05 indicates statistically significant differences\n",
    "\n",
    "### Interpretation Guidelines\n",
    "\n",
    "**\u2713 Model is production-ready if:**\n",
    "- Homology-aware accuracy > 75%\n",
    "- MobiDB accuracy > 70%\n",
    "- Label-shuffle accuracy ~ 50%\n",
    "- Narrow bootstrap confidence intervals\n",
    "\n",
    "**\u26a0 Model needs refinement if:**\n",
    "- Large accuracy drop in homology-aware split (>15% drop)\n",
    "- Poor MobiDB performance (<60% accuracy)\n",
    "- Label-shuffle accuracy > 60%\n",
    "- Wide bootstrap confidence intervals\n",
    "\n",
    "**Possible failure modes:**\n",
    "- **Dataset-specific biases**: Model captures PDB/DisProt artifacts rather than true folding propensity\n",
    "- **Sequence homology leakage**: Training and test sets contain similar sequences\n",
    "- **Feature artifacts**: Features correlate with dataset origin rather than disorder\n",
    "- **Overfitting**: Model memorizes training data rather than learning general patterns\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Based on validation results:\n",
    "1. If successful: Deploy concept model API with confidence\n",
    "2. If marginal: Investigate feature engineering and dataset curation\n",
    "3. If failed: Reconsider concept model approach or add ML components\n",
    "\n",
    "---\n",
    "\n",
    "*Note: To run all experiments, uncomment the function calls in the cells above.*\n",
    "*Some experiments (MobiDB fetch) may require significant time due to API rate limits.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}